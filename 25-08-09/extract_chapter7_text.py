#!/usr/bin/env python3
"""
Chapter 7 Text Extraction Script
í…ìŠ¤íŠ¸ ê²½ê³„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 7ì¥ì˜ ê° ë¦¬í”„ ë…¸ë“œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import json
import os

def load_json_file(file_path):
    """JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_text_file(file_path):
    """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def extract_text_between(content, start_text, end_text, section_title):
    """ì‹œì‘ ë¬¸ìì—´ê³¼ ì¢…ë£Œ ë¬¸ìì—´ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print(f"\n=== {section_title} ì¶”ì¶œ ì¤‘ ===")
    print(f"ì‹œì‘ ë¬¸ìì—´: '{start_text}'")
    print(f"ì¢…ë£Œ ë¬¸ìì—´: '{end_text}'")
    
    start_idx = content.find(start_text)
    if start_idx == -1:
        print(f"âŒ ì‹œì‘ ë¬¸ìì—´ '{start_text}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    print(f"âœ“ ì‹œì‘ ìœ„ì¹˜: {start_idx}")
    
    # ì‹œì‘ ìœ„ì¹˜ë¶€í„° ì¢…ë£Œ ë¬¸ìì—´ ê²€ìƒ‰
    end_idx = content.find(end_text, start_idx)
    if end_idx == -1:
        print(f"âŒ ì¢…ë£Œ ë¬¸ìì—´ '{end_text}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    print(f"âœ“ ì¢…ë£Œ ìœ„ì¹˜: {end_idx}")
    
    # ì¢…ë£Œ ë¬¸ìì—´ í¬í•¨í•˜ì—¬ ì¶”ì¶œ
    extracted_text = content[start_idx:end_idx + len(end_text)]
    print(f"âœ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_text)} ë¬¸ì")
    
    return extracted_text

def main():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    boundaries_file = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/chapter7_leaf_nodes_with_boundaries.json"
    source_text_file = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/extracted_texts/Level01_7 Basic data validation.md"
    output_dir = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/extracted_leaf_nodes/"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“ íŒŒì¼ ë¡œë“œ ì¤‘...")
    boundaries = load_json_file(boundaries_file)
    source_content = load_text_file(source_text_file)
    
    print(f"âœ“ ê²½ê³„ ì •ë³´: {len(boundaries)}ê°œ ë¦¬í”„ ë…¸ë“œ")
    print(f"âœ“ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(source_content)} ë¬¸ì")
    
    extracted_texts = {}
    
    # ê° ë¦¬í”„ ë…¸ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for node in boundaries:
        node_id = node['id']
        title = node['title']
        start_text = node['start_text']
        end_text = node['end_text']
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted = extract_text_between(source_content, start_text, end_text, title)
        
        if extracted:
            # íŒŒì¼ë¡œ ì €ì¥
            safe_title = title.replace('/', '_').replace(' ', '_')
            output_file = f"{output_dir}node_{node_id}_{safe_title}.txt"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"=== {title} (ID: {node_id}) ===\n\n")
                f.write(extracted)
            
            extracted_texts[node_id] = {
                'title': title,
                'text': extracted,
                'length': len(extracted),
                'file': output_file
            }
            
            print(f"ğŸ’¾ ì €ì¥ë¨: {output_file}")
            
        else:
            print(f"âŒ {title} ì¶”ì¶œ ì‹¤íŒ¨")
    
    # ì¶”ì¶œ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:")
    print(f"ì´ {len(boundaries)}ê°œ ë…¸ë“œ ì¤‘ {len(extracted_texts)}ê°œ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œ")
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥
    summary_file = f"{output_dir}extraction_summary.json"
    summary_data = {
        'total_nodes': len(boundaries),
        'extracted_nodes': len(extracted_texts),
        'extraction_results': extracted_texts
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‹ ì¶”ì¶œ ìš”ì•½ ì €ì¥: {summary_file}")
    
    return extracted_texts

if __name__ == "__main__":
    extracted_texts = main()