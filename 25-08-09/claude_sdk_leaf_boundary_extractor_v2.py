#!/usr/bin/env python3
"""
Claude Code SDK ê¸°ë°˜ ë¦¬í”„ ë…¸ë“œ ê²½ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° (Max Plan ì‚¬ìš©ììš©)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¦¬í”„ ë…¸ë“œ JSON íŒŒì¼ê³¼ ë§ˆí¬ë‹¤ìš´ ì›ë¬¸ì„ ì…ë ¥ë°›ì•„,
ê° ë¦¬í”„ ë…¸ë“œ ì„¹ì…˜ì˜ ì‹œì‘ê³¼ ë ë¶€ë¶„ì—ì„œ ì§€ì •ëœ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ìš”êµ¬ì‚¬í•­:
- Python 3.10+
- Node.js
- Claude Code: npm install -g @anthropic-ai/claude-code
- Python íŒ¨í‚¤ì§€: pip install claude-code-sdk

ì‚¬ìš©ë²•:
python claude_sdk_leaf_boundary_extractor_v2.py [leaf_nodes_json] [chapter_markdown] [output_json]
"""

import json
import asyncio
import sys
import re
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from claude_code_sdk import query, ClaudeCodeOptions, AssistantMessage, TextBlock
from claude_code_sdk import CLINotFoundError, ProcessError, CLIJSONDecodeError


class ClaudeSDKLeafBoundaryExtractor:
    """Claude Code SDKë¥¼ í™œìš©í•œ ë¦¬í”„ ë…¸ë“œ ê²½ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° (Max Plan ìµœì í™”)"""
    
    def __init__(self, extract_length: int = 15, debug: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            extract_length: ì‹œì‘/ëì—ì„œ ì¶”ì¶œí•  í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 200ì)
            debug: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
        """
        self.extract_length = extract_length
        self.debug = debug
        self.start_time = time.time()
        self.stats = {
            'processed_nodes': 0,
            'successful_nodes': 0,
            'failed_nodes': 0,
            'claude_api_calls': 0,
            'fallback_searches': 0,
            'claude_partial_matches': 0,
            'fallback_partial_matches': 0,
            'total_processing_time': 0,
            'text_length_analysis': {},
            'section_detection_analysis': {},
            'errors': []
        }
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # Max Plan ì‚¬ìš©ììš© ê°„ë‹¨í•œ ì˜µì…˜
        self.options = ClaudeCodeOptions(
            max_turns=1,
            system_prompt="JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” ê¸ˆì§€ì…ë‹ˆë‹¤."
        )
        
        self.logger.info(f"ğŸš€ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ - ì¶”ì¶œ ê¸¸ì´: {extract_length}ì, ë””ë²„ê·¸: {debug}")
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        # ë¡œê·¸ íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/logs/extractor_{timestamp}.log"
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}_{timestamp}")
        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)
        
        # í•¸ë“¤ëŸ¬ ì¤‘ë³µ ë°©ì§€
        if not self.logger.handlers:
            # íŒŒì¼ í•¸ë“¤ëŸ¬
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            
            # ì½˜ì†” í•¸ë“¤ëŸ¬
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # í¬ë§¤í„° ì„¤ì •
            detailed_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
            )
            simple_formatter = logging.Formatter('%(levelname)s - %(message)s')
            
            file_handler.setFormatter(detailed_formatter)
            console_handler.setFormatter(simple_formatter)
            
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)
        
        print(f"ğŸ“‹ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    def _log_progress(self, current: int, total: int, node_title: str):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        percentage = (current / total) * 100
        elapsed = time.time() - self.start_time
        eta = (elapsed / current) * (total - current) if current > 0 else 0
        
        self.logger.info(
            f"ì§„í–‰ë¥ : {percentage:.1f}% ({current}/{total}) | "
            f"ê²½ê³¼: {elapsed:.1f}s | ì˜ˆìƒ ì™„ë£Œ: {eta:.1f}s | "
            f"í˜„ì¬: {node_title}"
        )
    
    def _update_stats(self, success: bool, error: str = None):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['processed_nodes'] += 1
        if success:
            self.stats['successful_nodes'] += 1
        else:
            self.stats['failed_nodes'] += 1
            if error:
                self.stats['errors'].append({
                    'timestamp': datetime.now().isoformat(),
                    'error': error,
                    'node_count': self.stats['processed_nodes']
                })
    
    def _log_stats(self):
        """í˜„ì¬ í†µê³„ ë¡œê¹…"""
        self.logger.info(
            f"ğŸ“Š í˜„ì¬ í†µê³„ - ì„±ê³µ: {self.stats['successful_nodes']}, "
            f"ì‹¤íŒ¨: {self.stats['failed_nodes']}, "
            f"Claude API í˜¸ì¶œ: {self.stats['claude_api_calls']}, "
            f"í´ë°± ê²€ìƒ‰: {self.stats['fallback_searches']}"
        )
    
    async def load_files(self, leaf_nodes_path: str, chapter_path: str) -> Tuple[List[Dict], str]:
        """
        ë¦¬í”„ ë…¸ë“œ JSON íŒŒì¼ê³¼ ì±•í„° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            leaf_nodes_path: ë¦¬í”„ ë…¸ë“œ JSON íŒŒì¼ ê²½ë¡œ
            chapter_path: ì±•í„° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            tuple: (ë¦¬í”„ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸, ì±•í„° í…ìŠ¤íŠ¸)
        """
        try:
            # ë¦¬í”„ ë…¸ë“œ JSON ë¡œë“œ
            with open(leaf_nodes_path, 'r', encoding='utf-8') as f:
                all_nodes = json.load(f)
            
            # 7ì¥ ê´€ë ¨ ë…¸ë“œë§Œ í•„í„°ë§ (id 66-72)
            chapter7_nodes = [
                node for node in all_nodes 
                if isinstance(node.get('id'), int) and 66 <= node['id'] <= 72
            ]
            
            # ì±•í„° í…ìŠ¤íŠ¸ ë¡œë“œ
            with open(chapter_path, 'r', encoding='utf-8') as f:
                chapter_text = f.read()
            
            self.logger.info(f"âœ“ ë¦¬í”„ ë…¸ë“œ ë¡œë“œ ì™„ë£Œ: {len(all_nodes)}ê°œ (7ì¥: {len(chapter7_nodes)}ê°œ)")
            self.logger.info(f"âœ“ ì±•í„° í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(chapter_text):,}ì")
            
            return chapter7_nodes, chapter_text
            
        except FileNotFoundError as e:
            self.logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            raise
    
    async def process_nodes_with_claude(self, chapter_text: str, leaf_nodes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Claude SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë…¸ë“œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤ (ê°•í™”ëœ ë””ë²„ê¹…).
        
        Args:
            chapter_text: ì „ì²´ ì±•í„° í…ìŠ¤íŠ¸
            leaf_nodes: ë¦¬í”„ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ë¦¬í”„ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
            self.stats['text_length_analysis'] = {
                'original_length': len(chapter_text),
                'used_length': 0,
                'truncated': False
            }
            
            # ğŸ“ ì„¹ì…˜ ì‚¬ì „ ë¶„ì„
            section_analysis = self._analyze_sections_in_text(chapter_text, leaf_nodes)
            self.stats['section_detection_analysis'] = section_analysis
            
            self.logger.info(f"ğŸ“Š ì„¹ì…˜ ë¶„ì„ ê²°ê³¼:")
            for node_id, analysis in section_analysis.items():
                title = analysis.get('title', 'Unknown')
                found = analysis.get('found', False)
                positions = analysis.get('positions', [])
                self.logger.info(f"   [{node_id}] {title}: {'âœ“' if found else 'âœ—'} ({len(positions)} ìœ„ì¹˜)")
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™” (Max Plan ê³ ë ¤í•˜ë˜ ì„¹ì…˜ ë¶„ì„ ë°˜ì˜)
            max_text_length = self._calculate_optimal_text_length(chapter_text, section_analysis)
            limited_text = chapter_text[:max_text_length] if len(chapter_text) > max_text_length else chapter_text
            
            self.stats['text_length_analysis']['used_length'] = len(limited_text)
            self.stats['text_length_analysis']['truncated'] = len(chapter_text) > max_text_length
            
            # ğŸ¯ íƒ€ê²Ÿ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
            prompt = self._create_enhanced_batch_prompt(leaf_nodes, limited_text, section_analysis)
            
            self.logger.info(f"ğŸ¤– Claudeì—ê²Œ {len(leaf_nodes)}ê°œ ë…¸ë“œ ì¼ê´„ ìš”ì²­ (í…ìŠ¤íŠ¸: {len(limited_text):,}ì)")
            self.logger.debug(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt):,}ì")
            
            self.stats['claude_api_calls'] += 1
            
            responses = []
            async for message in query(prompt=prompt, options=self.options):
                if isinstance(message, AssistantMessage):
                    for block in message.content:
                        if isinstance(block, TextBlock):
                            responses.append(block.text)
            
            if responses:
                full_response = "\n".join(responses)
                self.logger.info(f"ğŸ“¥ Claude ì‘ë‹µ ê¸¸ì´: {len(full_response)} ë¬¸ì")
                self.logger.debug(f"ğŸ“ Claude ì‘ë‹µ ìƒ˜í”Œ: {full_response[:300]}...")
                
                # ê°•í™”ëœ ì‘ë‹µ íŒŒì‹±
                parsed_nodes = self._parse_claude_response_enhanced(full_response, leaf_nodes, section_analysis)
                if parsed_nodes:
                    success_count = len([n for n in parsed_nodes if n.get('start_text', '')])
                    self.logger.info(f"âœ… Claudeë¡œ {success_count}/{len(parsed_nodes)}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì„±ê³µ")
                    
                    # ë¶€ë¶„ ì‹¤íŒ¨ì‹œ ìë™ í´ë°± ì ìš©
                    if success_count < len(parsed_nodes):
                        self.logger.warning(f"âš ï¸ {len(parsed_nodes) - success_count}ê°œ ë…¸ë“œ ì‹¤íŒ¨, í´ë°±ìœ¼ë¡œ ë³´ì™„")
                        return await self._enhance_with_fallback(chapter_text, parsed_nodes)
                    
                    return parsed_nodes
            
            # Claude ì™„ì „ ì‹¤íŒ¨ ì‹œ í´ë°±
            self.logger.warning("âš ï¸ Claude ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì „ì²´ í´ë°± ê²€ìƒ‰ ì‚¬ìš©")
            return await self._process_nodes_fallback(chapter_text, leaf_nodes)
            
        except Exception as e:
            self.logger.error(f"Claude ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.debug(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            self.stats['errors'].append({
                'timestamp': datetime.now().isoformat(),
                'error': f"Claude processing error: {str(e)}",
                'context': 'batch_processing',
                'traceback': traceback.format_exc()
            })
            # í´ë°±ìœ¼ë¡œ ì²˜ë¦¬
            return await self._process_nodes_fallback(chapter_text, leaf_nodes)
    
    def _analyze_sections_in_text(self, text: str, leaf_nodes: List[Dict]) -> Dict:
        """í…ìŠ¤íŠ¸ ë‚´ ì„¹ì…˜ë“¤ì„ ì‚¬ì „ ë¶„ì„"""
        analysis = {}
        
        for node in leaf_nodes:
            node_id = node.get('id')
            title = node.get('title', '')
            
            # ë‹¤ì–‘í•œ ì œëª© íŒ¨í„´ ê²€ìƒ‰
            title_patterns = [
                title,  # ì›ë³¸
                title.replace("'", "'").replace("'", "'"),  # ë”°ì˜´í‘œ ì •ê·œí™”
                title.replace(""", '"').replace(""", '"'),  # ì¸ìš©ë¶€í˜¸ ì •ê·œí™”
                re.sub(r'\s+', ' ', title.strip()),  # ê³µë°± ì •ê·œí™”
                title.split()[0] if title.split() else title,  # ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ
            ]
            
            # ì¶”ê°€ íŒ¨í„´ë“¤ (7.1, 7.2 ë“±)
            if re.match(r'^\d+\.\d+', title):
                title_patterns.append(title.split()[0])  # "7.1" ë¶€ë¶„ë§Œ
            
            positions = []
            for pattern in title_patterns:
                if not pattern:
                    continue
                
                # ì •í™•í•œ ë§¤ì¹­
                start = 0
                while True:
                    pos = text.find(pattern, start)
                    if pos == -1:
                        break
                    positions.append({
                        'position': pos,
                        'pattern': pattern,
                        'context': text[max(0, pos-20):pos+len(pattern)+20]
                    })
                    start = pos + 1
            
            analysis[node_id] = {
                'title': title,
                'found': len(positions) > 0,
                'positions': positions,
                'patterns_tried': title_patterns
            }
        
        return analysis
    
    def _calculate_optimal_text_length(self, text: str, section_analysis: Dict) -> int:
        """ì„¹ì…˜ ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•œ ìµœì  í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°"""
        # ê¸°ë³¸ Max Plan ì œí•œ
        base_limit = 12000
        
        # ëª¨ë“  ì„¹ì…˜ì´ ë°œê²¬ëœ ë§ˆì§€ë§‰ ìœ„ì¹˜ ì°¾ê¸°
        last_found_pos = 0
        for analysis in section_analysis.values():
            if analysis['found'] and analysis['positions']:
                max_pos = max(pos['position'] for pos in analysis['positions'])
                last_found_pos = max(last_found_pos, max_pos)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì´í›„ ì¶©ë¶„í•œ ì—¬ìœ  ê³µê°„ ì¶”ê°€
        optimal_length = min(base_limit, last_found_pos + 5000)
        
        self.logger.debug(f"í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”: ë§ˆì§€ë§‰ ì„¹ì…˜ ìœ„ì¹˜ {last_found_pos}, ìµœì  ê¸¸ì´ {optimal_length}")
        return optimal_length
    
    def _create_enhanced_batch_prompt(self, leaf_nodes: List[Dict], text: str, section_analysis: Dict) -> str:
        """ê°•í™”ëœ ì¼ê´„ ì²˜ë¦¬ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ë°œê²¬ëœ ì„¹ì…˜ê³¼ ì‹¤íŒ¨í•œ ì„¹ì…˜ êµ¬ë¶„
        found_nodes = []
        missing_nodes = []
        
        for node in leaf_nodes:
            node_id = node.get('id')
            analysis = section_analysis.get(node_id, {})
            if analysis.get('found', False):
                found_nodes.append(node)
            else:
                missing_nodes.append(node)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = [
            f"í…ìŠ¤íŠ¸ì—ì„œ ê° ì œëª©ì˜ ì‹œì‘ê³¼ ë ë¶€ë¶„ì„ ì°¾ì•„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.",
            f"",
            f"í…ìŠ¤íŠ¸:",
            text,
            f"",
            f"ì°¾ì„ ë…¸ë“œë“¤ ({len(found_nodes)}ê°œ ë°œê²¬ë¨, {len(missing_nodes)}ê°œ ë¯¸ë°œê²¬):"
        ]
        
        # ë…¸ë“œ ì •ë³´ (ë°œê²¬ ìƒíƒœ í¬í•¨)
        for node in leaf_nodes:
            node_id = node.get('id')
            title = node.get('title', '')
            level = node.get('level', 1)
            analysis = section_analysis.get(node_id, {})
            found_status = "âœ“" if analysis.get('found', False) else "âœ—"
            
            prompt_parts.append(f"- [{node_id}] {title} (ë ˆë²¨ {level}) {found_status}")
        
        prompt_parts.extend([
            f"",
            f"ê° ë…¸ë“œì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:",
            f"{{",
            f'  "id": [ë…¸ë“œID],',
            f'  "title": "[ì œëª©]",',  
            f'  "level": [ë ˆë²¨],',
            f'  "start_text": "[ì„¹ì…˜ ì‹œì‘ë¶€ë¶„ {self.extract_length}ì]",',
            f'  "end_text": "[ì„¹ì…˜ ëë¶€ë¶„ {self.extract_length}ì]",',
            f'  "section_start_pos": [ì‹œì‘ìœ„ì¹˜],',
            f'  "section_end_pos": [ëìœ„ì¹˜]',
            f"}}",
            f"",
            f"JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì„¤ëª… ê¸ˆì§€."
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_claude_response_enhanced(self, response_text: str, original_nodes: List[Dict], section_analysis: Dict) -> Optional[List[Dict]]:
        """ê°•í™”ëœ Claude ì‘ë‹µ íŒŒì‹±"""
        try:
            self.logger.debug(f"ê°•í™”ëœ ì‘ë‹µ íŒŒì‹± ì‹œì‘: {response_text[:200]}...")
            
            # JSON ë¸”ë¡ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            json_patterns = [
                r'```json\s*([\s\S]*?)\s*```',  # í‘œì¤€ JSON ë¸”ë¡
                r'```\s*([\s\S]*?)\s*```',      # ì¼ë°˜ ì½”ë“œ ë¸”ë¡
                r'\[[\s\S]*?\]'                  # JSON ë°°ì—´ ì§ì ‘
            ]
            
            json_text = ""
            for pattern in json_patterns:
                matches = re.findall(pattern, response_text, re.IGNORECASE)
                if matches:
                    # ê°€ì¥ í° ë§¤ì¹˜ ì„ íƒ
                    json_text = max(matches, key=len).strip()
                    self.logger.debug(f"JSON íŒ¨í„´ '{pattern}' ë§¤ì¹˜ ì„±ê³µ")
                    break
            
            if json_text:
                parsed_data = json.loads(json_text)
                
                if isinstance(parsed_data, list) and len(parsed_data) >= 0:
                    self.logger.info(f"âœ… ê°•í™”ëœ JSON íŒŒì‹± ì„±ê³µ: {len(parsed_data)}ê°œ ë…¸ë“œ")
                    
                    # ì›ë³¸ ë…¸ë“œì™€ ë§¤ì¹­í•˜ì—¬ ë³´ì™„
                    return self._merge_with_original_nodes_enhanced(parsed_data, original_nodes, section_analysis)
                else:
                    self.logger.error("ì˜ëª»ëœ JSON êµ¬ì¡°")
                    return None
            else:
                self.logger.error("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                return self._extract_from_plain_text(response_text, original_nodes, section_analysis)
                
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ìˆ˜ë™ ì¶”ì¶œ ì‹œë„
            return self._extract_from_plain_text(response_text, original_nodes, section_analysis)
        except Exception as e:
            self.logger.error(f"ê°•í™”ëœ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_from_plain_text(self, response_text: str, original_nodes: List[Dict], section_analysis: Dict) -> Optional[List[Dict]]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í”Œë ˆì¸ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        self.logger.info("ğŸ”§ í”Œë ˆì¸ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ìˆ˜ë™ ì¶”ì¶œ ì‹œë„")
        
        results = []
        for node in original_nodes:
            node_id = node.get('id')
            title = node.get('title', '')
            
            # ì‘ë‹µì—ì„œ í•´ë‹¹ ë…¸ë“œ ì–¸ê¸‰ ì°¾ê¸°
            node_mentions = []
            for line in response_text.split('\n'):
                if str(node_id) in line or title in line:
                    node_mentions.append(line.strip())
            
            if node_mentions:
                self.logger.debug(f"ë…¸ë“œ {node_id} ì–¸ê¸‰ ë°œê²¬: {len(node_mentions)}ê±´")
                self.stats['claude_partial_matches'] += 1
            
            # ê¸°ë³¸ ë…¸ë“œë¡œ ì„¤ì • (ì¶”í›„ í´ë°±ìœ¼ë¡œ ë³´ì™„)
            result_node = node.copy()
            result_node.update({
                'start_text': '',
                'end_text': '',
                'section_start_pos': -1,
                'section_end_pos': -1,
                'claude_mentions': node_mentions
            })
            results.append(result_node)
        
        return results if results else None
    
    def _merge_with_original_nodes_enhanced(self, parsed_nodes: List[Dict], original_nodes: List[Dict], section_analysis: Dict) -> List[Dict]:
        """ê°•í™”ëœ ë…¸ë“œ ë³‘í•©"""
        result = []
        
        for original in original_nodes:
            node_id = original.get('id')
            original_title = original.get('title', '').strip()
            
            # íŒŒì‹±ëœ ë…¸ë“œì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸°
            matched = None
            for parsed in parsed_nodes:
                # ID ë§¤ì¹­
                if parsed.get('id') == node_id:
                    matched = parsed
                    break
                # ì œëª© ë§¤ì¹­ (ì •ê·œí™”)
                parsed_title = parsed.get('title', '').strip()
                if (parsed_title == original_title or
                    re.sub(r'\s+', ' ', parsed_title) == re.sub(r'\s+', ' ', original_title)):
                    matched = parsed
                    break
            
            if matched and matched.get('start_text', '').strip():
                # ë§¤ì¹­ ì„±ê³µ
                merged = original.copy()
                merged.update({
                    'start_text': matched.get('start_text', '')[:self.extract_length],
                    'end_text': matched.get('end_text', '')[:self.extract_length],
                    'section_start_pos': matched.get('section_start_pos', -1),
                    'section_end_pos': matched.get('section_end_pos', -1)
                })
                result.append(merged)
                self._update_stats(success=True)
                self.logger.debug(f"âœ“ ë…¸ë“œ {node_id} ë§¤ì¹­ ì„±ê³µ")
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ - í´ë°± ëŒ€ìƒ
                failed = original.copy()
                failed.update({
                    'start_text': '',
                    'end_text': '',
                    'section_start_pos': -1,
                    'section_end_pos': -1,
                    'needs_fallback': True,
                    'error': 'No matching data from Claude'
                })
                result.append(failed)
                self._update_stats(success=False, error='No matching Claude data')
                self.logger.debug(f"âœ— ë…¸ë“œ {node_id} ë§¤ì¹­ ì‹¤íŒ¨ - í´ë°± í•„ìš”")
        
        return result
    
    async def _enhance_with_fallback(self, chapter_text: str, partial_nodes: List[Dict]) -> List[Dict]:
        """ë¶€ë¶„ ì‹¤íŒ¨í•œ ë…¸ë“œë“¤ì„ í´ë°±ìœ¼ë¡œ ë³´ì™„"""
        self.logger.info("ğŸ”„ ë¶€ë¶„ ì‹¤íŒ¨ ë…¸ë“œë“¤ì„ í´ë°±ìœ¼ë¡œ ë³´ì™„ ì¤‘...")
        
        enhanced_nodes = []
        for node in partial_nodes:
            if node.get('needs_fallback', False) or not node.get('start_text', '').strip():
                # í´ë°± ì²˜ë¦¬ í•„ìš”
                try:
                    title = node.get('title', '')
                    start_pos, end_pos = self._fallback_search_enhanced(chapter_text, title, node.get('id'))
                    
                    if start_pos != -1:
                        start_text, end_text = self._extract_boundary_texts(chapter_text, start_pos, end_pos)
                        
                        enhanced = node.copy()
                        enhanced.update({
                            'start_text': start_text,
                            'end_text': end_text,
                            'section_start_pos': start_pos,
                            'section_end_pos': end_pos,
                            'processed_by': 'fallback',
                            'needs_fallback': False
                        })
                        enhanced_nodes.append(enhanced)
                        self.stats['fallback_partial_matches'] += 1
                        self.logger.debug(f"âœ“ í´ë°±ìœ¼ë¡œ ë…¸ë“œ {node.get('id')} ë³´ì™„ ì„±ê³µ")
                    else:
                        enhanced_nodes.append(node)
                        self.logger.warning(f"âœ— í´ë°±ìœ¼ë¡œë„ ë…¸ë“œ {node.get('id')} ì²˜ë¦¬ ì‹¤íŒ¨")
                        
                except Exception as e:
                    self.logger.error(f"í´ë°± ë³´ì™„ ì˜¤ë¥˜ (ë…¸ë“œ {node.get('id')}): {e}")
                    enhanced_nodes.append(node)
            else:
                # ì´ë¯¸ ì„±ê³µí•œ ë…¸ë“œ
                enhanced_nodes.append(node)
        
        return enhanced_nodes
    
    def _create_batch_prompt(self, leaf_nodes: List[Dict], text: str) -> str:
        """ì¼ê´„ ì²˜ë¦¬ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ë…¸ë“œ ì •ë³´ ê°„ë‹¨íˆ ì •ë¦¬
        node_info = []
        for node in leaf_nodes:
            node_info.append({
                'id': node.get('id'),
                'title': node.get('title', ''),
                'level': node.get('level', 1)
            })
        
        prompt = f"""í…ìŠ¤íŠ¸ì—ì„œ ê° ì œëª©ì˜ ì‹œì‘ê³¼ ë ë¶€ë¶„ì„ ì°¾ì•„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ì°¾ì„ ë…¸ë“œë“¤:
{json.dumps(node_info, ensure_ascii=False, indent=2)}

ê° ë…¸ë“œì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
{{
  "id": [ë…¸ë“œID],
  "title": "[ì œëª©]", 
  "level": [ë ˆë²¨],
  "start_text": "[ì„¹ì…˜ ì‹œì‘ë¶€ë¶„ {self.extract_length}ì]",
  "end_text": "[ì„¹ì…˜ ëë¶€ë¶„ {self.extract_length}ì]",
  "section_start_pos": [ì‹œì‘ìœ„ì¹˜],
  "section_end_pos": [ëìœ„ì¹˜]
}}

JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì„¤ëª… ê¸ˆì§€."""
        
        return prompt
    
    def _parse_claude_response(self, response_text: str, original_nodes: List[Dict]) -> Optional[List[Dict]]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        try:
            self.logger.debug(f"ì‘ë‹µ íŒŒì‹± ì‹œì‘: {response_text[:200]}...")
            
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_pattern = r'```json\s*([\s\S]*?)\s*```'
            json_matches = re.findall(json_pattern, response_text, re.IGNORECASE)
            
            json_text = ""
            if json_matches:
                json_text = json_matches[0].strip()
            else:
                # JSON ë°°ì—´ ì§ì ‘ ì°¾ê¸°
                array_pattern = r'\[[\s\S]*?\]'
                array_matches = re.findall(array_pattern, response_text)
                if array_matches:
                    json_text = max(array_matches, key=len).strip()
            
            if json_text:
                parsed_data = json.loads(json_text)
                
                if isinstance(parsed_data, list) and len(parsed_data) > 0:
                    self.logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: {len(parsed_data)}ê°œ ë…¸ë“œ")
                    
                    # ì›ë³¸ ë…¸ë“œì™€ ë§¤ì¹­í•˜ì—¬ ëˆ„ë½ëœ í•„ë“œ ë³´ì™„
                    return self._merge_with_original_nodes(parsed_data, original_nodes)
                else:
                    self.logger.error("ì˜ëª»ëœ JSON êµ¬ì¡°")
                    return None
            else:
                self.logger.error("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            self.logger.error(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _merge_with_original_nodes(self, parsed_nodes: List[Dict], original_nodes: List[Dict]) -> List[Dict]:
        """íŒŒì‹±ëœ ë…¸ë“œë¥¼ ì›ë³¸ê³¼ ë³‘í•©"""
        result = []
        
        for original in original_nodes:
            # íŒŒì‹±ëœ ë…¸ë“œì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸°
            matched = None
            for parsed in parsed_nodes:
                if (parsed.get('id') == original.get('id') or 
                    parsed.get('title', '').strip() == original.get('title', '').strip()):
                    matched = parsed
                    break
            
            if matched:
                # ì›ë³¸ ë…¸ë“œë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³  íŒŒì‹±ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
                merged = original.copy()
                merged.update({
                    'start_text': matched.get('start_text', ''),
                    'end_text': matched.get('end_text', ''),
                    'section_start_pos': matched.get('section_start_pos', -1),
                    'section_end_pos': matched.get('section_end_pos', -1)
                })
                result.append(merged)
                self._update_stats(success=True)
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
                failed = original.copy()
                failed.update({
                    'start_text': '',
                    'end_text': '',
                    'section_start_pos': -1,
                    'section_end_pos': -1,
                    'error': 'No matching data from Claude'
                })
                result.append(failed)
                self._update_stats(success=False, error='No matching Claude data')
        
        return result
    
    async def _process_nodes_fallback(self, chapter_text: str, leaf_nodes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í´ë°±: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬"""
        self.logger.info(f"ğŸ”„ í´ë°± ê²€ìƒ‰ìœ¼ë¡œ {len(leaf_nodes)}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì¤‘...")
        self.stats['fallback_searches'] += 1
        
        updated_nodes = []
        
        for node in leaf_nodes:
            try:
                title = node.get('title', '')
                
                # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                start_pos, end_pos = self._fallback_search(chapter_text, title)
                
                # ê²½ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                start_text, end_text = self._extract_boundary_texts(chapter_text, start_pos, end_pos)
                
                # ë…¸ë“œ ì—…ë°ì´íŠ¸
                updated_node = node.copy()
                updated_node.update({
                    'start_text': start_text,
                    'end_text': end_text,
                    'section_start_pos': start_pos,
                    'section_end_pos': end_pos
                })
                
                updated_nodes.append(updated_node)
                self._update_stats(success=True)
                
                self.logger.debug(f"âœ“ í´ë°± ì²˜ë¦¬ ì„±ê³µ: {title}")
                
            except Exception as e:
                self.logger.error(f"í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {node.get('title', 'Unknown')} - {e}")
                error_node = node.copy()
                error_node.update({
                    'start_text': '',
                    'end_text': '',
                    'section_start_pos': -1,
                    'section_end_pos': -1,
                    'error': str(e)
                })
                updated_nodes.append(error_node)
                self._update_stats(success=False, error=str(e))
        
        return updated_nodes
    
    def _fallback_search_enhanced(self, text: str, title: str, node_id: int) -> Tuple[int, int]:
        """ê°•í™”ëœ í´ë°± ê²€ìƒ‰ (ë” ë§ì€ íŒ¨í„´ ì‹œë„)"""
        self.logger.debug(f"ê°•í™”ëœ í´ë°± ê²€ìƒ‰ ì‹œì‘: [{node_id}] {title}")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ë“¤
        title_variants = [
            title,  # ì›ë³¸
            title.replace("'", "'").replace("'", "'"),  # ë”°ì˜´í‘œ ì •ê·œí™”
            title.replace(""", '"').replace(""", '"'),  # ì¸ìš©ë¶€í˜¸ ì •ê·œí™”
            re.sub(r'\s+', ' ', title.strip()),  # ê³µë°± ì •ê·œí™”
        ]
        
        # 2ë‹¨ê³„: ì„¹ì…˜ ë²ˆí˜¸ íŒ¨í„´ (7.4, 7.5 ë“±)
        if re.match(r'^\d+\.\d+', title):
            section_num = title.split()[0]  # "7.4" ë¶€ë¶„
            title_variants.extend([
                section_num,
                f"\n{section_num}",  # ì¤„ë°”ê¿ˆ í›„ ì„¹ì…˜ ë²ˆí˜¸
                f"=== í˜ì´ì§€.*{section_num}",  # í˜ì´ì§€ í‘œì‹œì™€ í•¨ê»˜
                f"{section_num}.*\n"  # ì„¹ì…˜ ë²ˆí˜¸ í›„ ì¤„ë°”ê¿ˆ
            ])
        
        # 3ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        title_words = title.split()
        if len(title_words) > 1:
            # ì£¼ìš” í‚¤ì›Œë“œë“¤
            for word in title_words[1:]:  # ì²« ë²ˆì§¸ëŠ” ë³´í†µ ë²ˆí˜¸
                if len(word) > 3:  # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ
                    title_variants.append(word)
        
        # 4ë‹¨ê³„: ì •ê·œì‹ íŒ¨í„´ ê²€ìƒ‰
        start_pos = -1
        found_pattern = None
        
        for variant in title_variants:
            if not variant:
                continue
            
            # ì •í™•í•œ ë¬¸ìì—´ ë§¤ì¹­
            pos = text.find(variant)
            if pos != -1:
                start_pos = pos
                found_pattern = variant
                break
                
            # ì •ê·œì‹ ë§¤ì¹­ (íŠ¹ìˆ˜ íŒ¨í„´)
            if "===" in variant or ".*" in variant:
                try:
                    pattern = variant
                    matches = re.search(pattern, text, re.MULTILINE)
                    if matches:
                        start_pos = matches.start()
                        found_pattern = variant
                        break
                except re.error:
                    continue
        
        if start_pos == -1:
            self.logger.warning(f"'{title}' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì‹œë„ëœ íŒ¨í„´: {len(title_variants)}ê°œ)")
            return 0, len(text)
        
        self.logger.debug(f"'{title}' ë°œê²¬: ìœ„ì¹˜ {start_pos}, íŒ¨í„´ '{found_pattern}'")
        
        # ì„¹ì…˜ ë ì°¾ê¸° (ê°•í™”ëœ ë§ˆì»¤)
        next_section_markers = [
            '\n# ',
            '\n## ',
            '\n### ',
            '\n#### ',
            f'\n{section_num[0]}.{int(section_num.split(".")[1]) + 1}' if re.match(r'^\d+\.\d+', title) else None,  # ë‹¤ìŒ ì„¹ì…˜ ë²ˆí˜¸
            '\n=== í˜ì´ì§€',
            '\nSummary',
            '\nìš”ì•½',
            '\n\n## ',  # ë‹¤ë¥¸ ì±•í„°
            '\nListing\d+\.\d+',  # ì½”ë“œ ë¦¬ìŠ¤íŒ…
        ]
        
        # None ì œê±°
        next_section_markers = [m for m in next_section_markers if m is not None]
        
        end_pos = len(text)
        search_start = start_pos + len(found_pattern) + 50
        
        for marker in next_section_markers:
            if marker.startswith('\n'):
                # ì •í™•í•œ ë¬¸ìì—´ ê²€ìƒ‰
                next_pos = text.find(marker, search_start)
            else:
                # ì •ê·œì‹ ê²€ìƒ‰
                try:
                    match = re.search(marker, text[search_start:], re.MULTILINE)
                    next_pos = search_start + match.start() if match else -1
                except re.error:
                    next_pos = -1
            
            if next_pos != -1:
                end_pos = min(end_pos, next_pos)
        
        self.logger.debug(f"'{title}' ì„¹ì…˜ ë²”ìœ„: {start_pos}-{end_pos} ({end_pos-start_pos}ì)")
        return start_pos, end_pos
    
    def _fallback_search(self, text: str, title: str) -> Tuple[int, int]:
        """ê°„ë‹¨í•œ í´ë°± ê²€ìƒ‰"""
        # ì œëª© ì •ê·œí™”
        title_variants = [
            title,
            title.replace("'", "'"),
            title.replace("'", "'"),
            title.replace(""", '"'),
            title.replace(""", '"'),
            re.sub(r'\s+', ' ', title.strip())
        ]
        
        start_pos = -1
        for variant in title_variants:
            start_pos = text.find(variant)
            if start_pos != -1:
                break
        
        if start_pos == -1:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            title_words = title.split()
            if title_words:
                start_pos = text.find(title_words[0])
        
        if start_pos == -1:
            self.logger.warning(f"'{title}' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return 0, len(text)
        
        # ë‹¤ìŒ ì„¹ì…˜ê¹Œì§€ ì°¾ê¸°
        next_section_markers = [
            '\n# ',
            '\n## ',
            '\n### ',
            '\n#### ',
            '\n=== í˜ì´ì§€',
            '\nSummary',
            '\nìš”ì•½'
        ]
        
        end_pos = len(text)
        search_start = start_pos + len(title) + 50
        
        for marker in next_section_markers:
            next_pos = text.find(marker, search_start)
            if next_pos != -1:
                end_pos = min(end_pos, next_pos)
        
        self.logger.debug(f"'{title}' ì„¹ì…˜ ë°œê²¬: {start_pos}-{end_pos}")
        return start_pos, end_pos
    
    def _extract_boundary_texts(self, text: str, start_pos: int, end_pos: int) -> Tuple[str, str]:
        """ê²½ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ì‹œì‘ í…ìŠ¤íŠ¸
        start_text = text[start_pos:start_pos + self.extract_length].strip()
        
        # ë í…ìŠ¤íŠ¸
        end_start = max(end_pos - self.extract_length, start_pos)
        end_text = text[end_start:end_pos].strip()
        
        return start_text, end_text
    
    async def save_results(self, updated_nodes: List[Dict[str, Any]], output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(updated_nodes, f, ensure_ascii=False, indent=2)
            
            # ìµœì¢… í†µê³„ ê³„ì‚°
            success_count = len([n for n in updated_nodes if n.get('start_text', '')])
            error_count = len([n for n in updated_nodes if n.get('error')])
            
            self.logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            self.logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
            self.logger.info(f"   - ì„±ê³µ: {success_count}ê°œ")
            self.logger.info(f"   - ì‹¤íŒ¨: {error_count}ê°œ")
            self.logger.info(f"   - ì´í•©: {len(updated_nodes)}ê°œ")
            self.logger.info(f"   - Claude API í˜¸ì¶œ: {self.stats['claude_api_calls']}íšŒ")
            self.logger.info(f"   - í´ë°± ê²€ìƒ‰: {self.stats['fallback_searches']}íšŒ")
            
            # ì´ ì²˜ë¦¬ ì‹œê°„
            total_time = time.time() - self.start_time
            self.logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Claude Code SDK ë¦¬í”„ ë…¸ë“œ ê²½ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° (Max Plan ìµœì í™”)")
    print("=" * 70)
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) == 4:
        leaf_nodes_path = sys.argv[1]
        chapter_path = sys.argv[2] 
        output_path = sys.argv[3]
    else:
        # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ
        leaf_nodes_path = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/part2_scalability_leaf_nodes.json"
        chapter_path = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/extracted_texts/Level01_7 Basic data validation.md"
        output_path = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-09/chapter7_leaf_nodes_with_boundaries_v2.json"
    
    # ì„¤ì •
    extractor = ClaudeSDKLeafBoundaryExtractor(extract_length=15, debug=True)
    
    try:
        # 1. íŒŒì¼ ë¡œë“œ
        print("\nğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘...")
        leaf_nodes, chapter_text = await extractor.load_files(
            leaf_nodes_path, chapter_path
        )
        
        if not leaf_nodes:
            print("âš ï¸ 7ì¥ ê´€ë ¨ ë¦¬í”„ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 1
        
        # 2. Claudeë¡œ ì¼ê´„ ì²˜ë¦¬ (Max Plan ìµœì í™”)
        print(f"\nğŸ¤– Claude SDKë¡œ {len(leaf_nodes)}ê°œ ë…¸ë“œ ì¼ê´„ ì²˜ë¦¬ ì¤‘...")
        updated_nodes = await extractor.process_nodes_with_claude(
            chapter_text, leaf_nodes
        )
        
        # 3. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        await extractor.save_results(updated_nodes, output_path)
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"   - ì…ë ¥: {leaf_nodes_path}")
        print(f"   - ì›ë¬¸: {chapter_path}")
        print(f"   - ì¶œë ¥: {output_path}")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return 1
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    exit_code = asyncio.run(main())
    exit(exit_code)


"""
Max Plan ì‚¬ìš©ì ìµœì í™” ë²„ì „ íŠ¹ì§•:

1. ì¼ê´„ ì²˜ë¦¬ ë°©ì‹:
   - ëª¨ë“  ë…¸ë“œë¥¼ í•œ ë²ˆì˜ Claude API í˜¸ì¶œë¡œ ì²˜ë¦¬
   - ë¹„ìš© íš¨ìœ¨ì„± ê·¹ëŒ€í™”

2. ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸:
   - "JSONë§Œ ë°˜í™˜" ì˜µì…˜ìœ¼ë¡œ í† í° ì ˆì•½
   - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°

3. ê°•ë ¥í•œ í´ë°± ì‹œìŠ¤í…œ:
   - Claude ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
   - 100% ì²˜ë¦¬ ë³´ì¥

4. ìƒì„¸í•œ ëª¨ë‹ˆí„°ë§:
   - ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ë° í†µê³„
   - ë¡œê·¸ íŒŒì¼ë¡œ ë””ë²„ê¹… ì§€ì›
   - ì„±ê³µ/ì‹¤íŒ¨ìœ¨ ì¶”ì 

5. ì˜¤ë¥˜ ë³µêµ¬:
   - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ìµœëŒ€í•œ ê²°ê³¼ ìƒì„±
   - ê° ë…¸ë“œë³„ ì˜¤ë¥˜ ì •ë³´ ê¸°ë¡

ì‚¬ìš© ë°©ë²•:
# ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
python claude_sdk_leaf_boundary_extractor_v2.py

# ì‚¬ìš©ì ì§€ì • ê²½ë¡œ
python claude_sdk_leaf_boundary_extractor_v2.py [ë¦¬í”„ë…¸ë“œJSON] [ì›ë¬¸MD] [ì¶œë ¥JSON]
"""