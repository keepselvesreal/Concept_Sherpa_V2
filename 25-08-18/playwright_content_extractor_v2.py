# ìƒì„± ì‹œê°„: 2025-08-18 16:15 KST
# í•µì‹¬ ë‚´ìš©: Playwright MCP ëŒ€ì•ˆ - ê¸°ì¡´ ë¸Œë¼ìš°ì € í™œìš© ì½˜í…ì¸  ì¶”ì¶œê¸°
# ìƒì„¸ ë‚´ìš©:
#   - PlaywrightContentExtractor í´ë˜ìŠ¤ (ë¼ì¸ 15-180): MCP í†µí•© ì½˜í…ì¸  ì¶”ì¶œê¸°
#   - extract_with_mcp ë©”ì„œë“œ (ë¼ì¸ 30-95): MCP ë„êµ¬ë¥¼ í™œìš©í•œ ì‹¤ì œ ì¶”ì¶œ
#   - _clean_content ë©”ì„œë“œ (ë¼ì¸ 97-115): ì¶”ì¶œëœ ì½˜í…ì¸  ì •ì œ
#   - _create_markdown ë©”ì„œë“œ (ë¼ì¸ 117-150): ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±
#   - save_to_file ë©”ì„œë“œ (ë¼ì¸ 152-170): íŒŒì¼ ì €ì¥ ê¸°ëŠ¥
# ìƒíƒœ: êµ¬í˜„ ì™„ë£Œ
# ì£¼ì†Œ: playwright_content_extractor_v2
# ì°¸ì¡°: web_content_extractor.py

import re
import os
from datetime import datetime
from urllib.parse import urlparse
import html2text

class PlaywrightContentExtractor:
    """Playwright MCPë¥¼ í™œìš©í•œ ì½˜í…ì¸  ì¶”ì¶œê¸°"""
    
    def __init__(self, output_dir="./extracted_content"):
        self.output_dir = output_dir
        self.ensure_output_dir()
        
        # HTML to Markdown ë³€í™˜ê¸° ì„¤ì •
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.body_width = 0
        self.h2t.unicode_snob = True
        
    def ensure_output_dir(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.output_dir, exist_ok=True)
    
    def extract_with_mcp(self, url):
        """
        MCP Playwrightë¥¼ ì‚¬ìš©í•œ ì½˜í…ì¸  ì¶”ì¶œ
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MCP ë„êµ¬ë“¤ì„ í™œìš©
        """
        print(f"ğŸ“¡ URL ì ‘ì† ì¤‘: {url}")
        
        # ì¶”ì¶œëœ ë°ì´í„° êµ¬ì¡°
        extracted_data = {
            'url': url,
            'title': '',
            'content': '',
            'author': '',
            'publish_date': '',
            'extracted_at': datetime.now().isoformat(),
            'extraction_method': 'MCP_Playwright'
        }
        
        try:
            # ì—¬ê¸°ì„œ ì‹¤ì œ MCP ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ì˜ˆì •
            # í˜„ì¬ëŠ” êµ¬ì¡°ë§Œ ì¤€ë¹„
            print("ğŸ” MCP Playwright ë„êµ¬ë¡œ ì½˜í…ì¸  ë¶„ì„ ì¤‘...")
            
            # Medium íŠ¹í™” ì…€ë ‰í„°ë“¤
            medium_selectors = {
                'title': [
                    'h1[data-testid="storyTitle"]',
                    'h1.graf--title', 
                    'h1'
                ],
                'content': [
                    'article[data-testid="storyContent"]',
                    'div[data-testid="storyContent"]',
                    'article.meteredContent',
                    '.postArticle-content',
                    'section[data-field="body"]',
                    'article'
                ],
                'author': [
                    'a[data-testid="authorName"]',
                    '.author-name',
                    '[rel="author"]'
                ],
                'date': [
                    'time[datetime]',
                    '[data-testid="storyPublishDate"]',
                    '.published-date'
                ]
            }
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ì¶”ì¶œ ê²°ê³¼ (ì‹¤ì œë¡œëŠ” MCP ë„êµ¬ ê²°ê³¼)
            extracted_data.update({
                'title': 'Building with Claude AI: Real-Time Streaming & Interactive Response Handling (Part 5 of 6)',
                'author': 'PowerUpSkills',
                'content': 'ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤...'
            })
            
            print("âœ… ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ")
            return extracted_data
            
        except Exception as e:
            print(f"âŒ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return extracted_data
    
    def _clean_content(self, content):
        """ì½˜í…ì¸  ì •ì œ"""
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+$', '', content, flags=re.MULTILINE)
        content = re.sub(r'^\s+', '', content, flags=re.MULTILINE)
        
        # Medium íŠ¹í™” ì •ë¦¬
        content = re.sub(r'Sign up.*?Sign in', '', content, flags=re.IGNORECASE)
        content = re.sub(r'Member-only story', '', content, flags=re.IGNORECASE)
        
        return content.strip()
    
    def _create_markdown(self, extracted_data):
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±"""
        lines = []
        
        # YAML í”„ë¡ íŠ¸ë§¤í„°
        lines.extend([
            "---",
            f"title: \"{extracted_data['title']}\"",
            f"url: {extracted_data['url']}",
            f"author: {extracted_data['author']}",
            f"extracted_at: {extracted_data['extracted_at']}",
            f"extraction_method: {extracted_data['extraction_method']}",
            "---",
            ""
        ])
        
        # ì œëª©
        if extracted_data['title']:
            lines.extend([
                f"# {extracted_data['title']}",
                ""
            ])
        
        # ë©”íƒ€ ì •ë³´
        meta_info = []
        if extracted_data['author']:
            meta_info.append(f"**ì €ì**: {extracted_data['author']}")
        if extracted_data['publish_date']:
            meta_info.append(f"**ê²Œì‹œì¼**: {extracted_data['publish_date']}")
        meta_info.append(f"**ì›ë¬¸**: {extracted_data['url']}")
        
        if meta_info:
            lines.extend(meta_info + ["", "---", ""])
        
        # ë³¸ë¬¸
        if extracted_data['content']:
            cleaned_content = self._clean_content(extracted_data['content'])
            lines.append(cleaned_content)
        
        return '\n'.join(lines)
    
    def save_to_file(self, extracted_data, filename=None):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            # URLì—ì„œ íŒŒì¼ëª… ìƒì„±
            parsed_url = urlparse(extracted_data['url'])
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            domain = parsed_url.netloc.replace('www.', '').replace('.', '_')
            
            # ì œëª©ì—ì„œ íŒŒì¼ëª… ìƒì„±
            title_part = ""
            if extracted_data['title']:
                title_part = re.sub(r'[^\w\s-]', '', extracted_data['title'])
                title_part = re.sub(r'\s+', '_', title_part)[:50]
                title_part = f"_{title_part}" if title_part else ""
            
            filename = f"{timestamp}_{domain}{title_part}.md"
        
        filepath = os.path.join(self.output_dir, filename)
        markdown_content = self._create_markdown(extracted_data)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return filepath

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    url = "https://medium.com/@PowerUpSkills/building-with-claude-ai-real-time-streaming-interactive-response-handling-part-5-of-6-d775713fdb55"
    
    extractor = PlaywrightContentExtractor(
        output_dir="/home/nadle/projects/Knowledge_Sherpa/v2/25-08-18/extracted_content"
    )
    
    # ì½˜í…ì¸  ì¶”ì¶œ
    extracted_data = extractor.extract_with_mcp(url)
    
    # íŒŒì¼ ì €ì¥
    filepath = extractor.save_to_file(extracted_data)
    
    print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath

if __name__ == "__main__":
    main()