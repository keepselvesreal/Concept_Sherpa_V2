"""
ìƒì„± ì‹œê°„: 2025-08-14 11:35:00 KST
í•µì‹¬ ë‚´ìš©: ì •ë°˜í•© ë°©ë²•ë¡  V5 - Node í´ë˜ìŠ¤ ì˜ì¡´ì„± ì™„ì „ ì œê±°
ìƒì„¸ ë‚´ìš©:
    - DataLoader í´ë˜ìŠ¤ (ë¼ì¸ 35-): ë”•ì…”ë„ˆë¦¬ ë…¸ë“œ ë°ì´í„° ì§ì ‘ ì²˜ë¦¬
    - DataProcessor í´ë˜ìŠ¤ (ë¼ì¸ 180-): ë°ì´í„° ê°€ê³µ ë° ê°±ì‹  ì‘ì—… ë¶„ë¦¬
    - DataSaver í´ë˜ìŠ¤ (ë¼ì¸ 320-): ê²°ê³¼ ì €ì¥ ì „ë‹´
    - DialecticalSynthesisProcessor í´ë˜ìŠ¤ (ë¼ì¸ 380-): ë©”ì¸ ì²˜ë¦¬ ë¡œì§ ì¡°ì§í™”
    - NodeGrouper ë¶„ë¦¬: ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ JSON ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ì²˜ë¦¬
    - Node í´ë˜ìŠ¤ ì˜ì¡´ì„± ì™„ì „ ì œê±°ë¡œ ì„±ëŠ¥ í–¥ìƒ
ìƒíƒœ: í™œì„±
ì£¼ì†Œ: dialectical_synthesis_processor_v5
ì°¸ì¡°: dialectical_synthesis_processor_v4.py (ì´ì „ ë²„ì „)
"""

import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any

from node_grouper import NodeGrouper
from content_analysis_module import ContentAnalyzer
from logging_system import ProcessLogger


class DataLoader:
    """ë…¸ë“œ ì •ë³´ íŒŒì¼ ë¡œë”© ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: Path, logger: ProcessLogger):
        self.base_dir = base_dir
        self.logger = logger
    
    def load_node_content_data(self, node_data: Dict[str, Any]) -> str:
        """gather_and_analyze_stageìš©: ë‚´ìš© ì˜ì—­ ë°ì´í„° ë¡œë”©"""
        try:
            node_title = node_data.get("title", "")
            
            # 1. ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ
            node_file_path = self._get_node_file_path(node_data)
            if not node_file_path.exists():
                self.logger.log_error(f"ë…¸ë“œíŒŒì¼ì—†ìŒ_{node_title}", f"íŒŒì¼ ì—†ìŒ: {node_file_path}")
                return ""
            
            # 2. í˜„ì¬ íŒŒì¼ì˜ ë‚´ìš© ì˜ì—­ ì¶”ì¶œ
            current_content = self._extract_section_from_file(node_file_path, "ë‚´ìš©")
            
            # 3. êµ¬ì„± ì˜ì—­ì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            composition_files = self._get_composition_files(node_file_path)
            
            # 4. êµ¬ì„± íŒŒì¼ë“¤ì˜ ë‚´ìš© ì˜ì—­ ì¶”ì¶œí•˜ì—¬ ê²°í•©
            composition_content = []
            for file_name in composition_files:
                file_path = self.base_dir / file_name
                if file_path.exists():
                    content = self._extract_section_from_file(file_path, "ë‚´ìš©")
                    if content:
                        composition_content.append(f"=== {file_name} ===\n{content}")
            
            # 5. ì „ì²´ ê²°í•©
            combined_text = self._combine_content_data(current_content, composition_content, node_title)
            
            self.logger.log_operation(f"ë‚´ìš©ë°ì´í„°ë¡œë”©_{node_title}", "ì„±ê³µ", 
                                    {"êµ¬ì„±íŒŒì¼ìˆ˜": len(composition_files), "í…ìŠ¤íŠ¸ê¸¸ì´": len(combined_text)})
            return combined_text
            
        except Exception as e:
            self.logger.log_error(f"ë‚´ìš©ë°ì´í„°ë¡œë”©_{node_data.get('title', '')}", e)
            return ""
    
    def load_node_extraction_data(self, node_data: Dict[str, Any]) -> str:
        """improve_individual_stageìš©: ì¶”ì¶œ ì˜ì—­ ë°ì´í„° ë¡œë”©"""
        try:
            node_title = node_data.get("title", "")
            
            # 1. ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ
            node_file_path = self._get_node_file_path(node_data)
            if not node_file_path.exists():
                return ""
            
            # 2. í˜„ì¬ íŒŒì¼ì˜ ì¶”ì¶œ ì˜ì—­ ì¶”ì¶œ
            current_extraction = self._extract_section_from_file(node_file_path, "ì¶”ì¶œ")
            
            # 3. êµ¬ì„± íŒŒì¼ë“¤ì˜ ì¶”ì¶œ ì˜ì—­ë„ ìˆ˜ì§‘
            composition_files = self._get_composition_files(node_file_path)
            composition_extractions = []
            
            for file_name in composition_files:
                file_path = self.base_dir / file_name
                if file_path.exists():
                    extraction = self._extract_section_from_file(file_path, "ì¶”ì¶œ")
                    if extraction:
                        composition_extractions.append(f"=== {file_name} ===\n{extraction}")
            
            # 4. ì „ì²´ ê²°í•©
            combined_extraction = self._combine_extraction_data(current_extraction, composition_extractions, node_title)
            
            self.logger.log_operation(f"ì¶”ì¶œë°ì´í„°ë¡œë”©_{node_title}", "ì„±ê³µ", 
                                    {"êµ¬ì„±íŒŒì¼ìˆ˜": len(composition_files), "í…ìŠ¤íŠ¸ê¸¸ì´": len(combined_extraction)})
            return combined_extraction
            
        except Exception as e:
            self.logger.log_error(f"ì¶”ì¶œë°ì´í„°ë¡œë”©_{node_data.get('title', '')}", e)
            return ""
    
    def _get_node_file_path(self, node_data: Dict[str, Any]) -> Path:
        """ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìƒì„± - {id:02d}_lev{level}_{title}_info.md í˜•ì‹"""
        node_id = node_data.get("id", 0)
        level = node_data.get("level", 0)
        title = node_data.get("title", "")
        safe_title = title.replace(" ", "_").replace("/", "_").replace("\\", "_")
        return self.base_dir / f"{node_id:02d}_lev{level}_{safe_title}_info.md"
    
    def _extract_section_from_file(self, file_path: Path, section_name: str) -> str:
        """íŒŒì¼ì—ì„œ íŠ¹ì • ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì„¹ì…˜ í—¤ë” ì°¾ê¸° (# ì„¹ì…˜ëª…)
            header_pattern = f"# {section_name}"
            header_start = content.find(header_pattern)
            
            if header_start == -1:
                return ""
            
            # ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ
            content_start = header_start + len(header_pattern)
            next_header_start = content.find("\n# ", content_start)
            
            if next_header_start == -1:
                section_content = content[content_start:].strip()
            else:
                section_content = content[content_start:next_header_start].strip()
            
            return section_content
            
        except Exception as e:
            self.logger.log_error(f"ì„¹ì…˜ì¶”ì¶œ_{file_path.name}", e)
            return ""
    
    def _get_composition_files(self, node_file_path: Path) -> List[str]:
        """êµ¬ì„± ì„¹ì…˜ì—ì„œ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ"""
        try:
            composition_content = self._extract_section_from_file(node_file_path, "êµ¬ì„±")
            if not composition_content:
                return []
            
            # íŒŒì¼ëª… ë¼ì¸ë“¤ ì¶”ì¶œ
            files = []
            for line in composition_content.split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and line.endswith('.md'):
                    files.append(line)
            
            return files
            
        except Exception:
            return []
    
    def _combine_content_data(self, current_content: str, composition_content: List[str], node_title: str) -> str:
        """ë‚´ìš© ì˜ì—­ ë°ì´í„° ê²°í•©"""
        combined = f"# {node_title} - ë‚´ìš© ì˜ì—­ í†µí•©\n\n"
        
        if current_content:
            combined += f"## í˜„ì¬ ë…¸ë“œ ë‚´ìš©\n{current_content}\n\n"
        
        if composition_content:
            combined += "## êµ¬ì„± ìš”ì†Œë“¤ ë‚´ìš©\n\n"
            for content in composition_content:
                combined += f"{content}\n\n"
        
        return combined
    
    def _combine_extraction_data(self, current_extraction: str, composition_extractions: List[str], node_title: str) -> str:
        """ì¶”ì¶œ ì˜ì—­ ë°ì´í„° ê²°í•©"""
        combined = f"# {node_title} - ì¶”ì¶œ ì˜ì—­ í†µí•©\n\n"
        
        if current_extraction:
            combined += f"## í˜„ì¬ ë…¸ë“œ ì¶”ì¶œ\n{current_extraction}\n\n"
        
        if composition_extractions:
            combined += "## êµ¬ì„± ìš”ì†Œë“¤ ì¶”ì¶œ\n\n"
            for extraction in composition_extractions:
                combined += f"{extraction}\n\n"
        
        return combined


class DataProcessor:
    """ë°ì´í„° ê°€ê³µ ë° ê°±ì‹  ì‘ì—… ì „ë‹´ í´ë˜ìŠ¤ - ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
    
    def __init__(self, content_analyzer: ContentAnalyzer, logger: ProcessLogger):
        self.content_analyzer = content_analyzer
        self.logger = logger
        self.semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
    
    async def process_content_extraction(self, combined_content: str, node_title: str) -> Dict[str, str]:
        """
        ë°ì´í„° ê°€ê³µ: í•µì‹¬, ìƒì„¸ í•µì‹¬, ì£¼ìš” í™”ì œ, ë¶€ì°¨ í™”ì œ ë³‘ë ¬ ì¶”ì¶œ
        ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ìš© ì˜ì—­ê³¼ êµ¬ì„± ì˜ì—­ì´ ê²°í•©ëœ ì •ë³´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰
        """
        process_start = time.time()
        self.logger.log_operation(f"ë‚´ìš©ì¶”ì¶œì‹œì‘_{node_title}", "ì‹œì‘")
        
        # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ ë¶„ì„ (analyze_contentì˜ ê¸°ë³¸ ë™ì‘ í™œìš©)
        try:
            # ContentAnalyzerì˜ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš© - combined íƒ€ì…ìœ¼ë¡œ ë¶„ì„
            result = await self.content_analyzer.analyze_content(
                content=combined_content,
                title=node_title,
                context_type="combined"  # ê¸°ì¡´ v2ì—ì„œ ì‚¬ìš©í•˜ë˜ íƒ€ì…
            )
            
            # ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬
            extracted_data = {}
            section_names = ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]
            
            for section_name in section_names:
                content = result.get(section_name, "")
                if content and not content.startswith("ë¶„ì„ ì‹¤íŒ¨") and len(content.strip()) > 0:
                    extracted_data[section_name] = content
                else:
                    extracted_data[section_name] = "ì¶”ì¶œ ì‹¤íŒ¨"
            
            process_time = time.time() - process_start
            success_count = len([v for v in extracted_data.values() if v != "ì¶”ì¶œ ì‹¤íŒ¨"])
            
            self.logger.log_operation(f"ë‚´ìš©ì¶”ì¶œì™„ë£Œ_{node_title}", "ì™„ë£Œ", 
                                    {"ì²˜ë¦¬ì‹œê°„": f"{process_time:.2f}ì´ˆ", "ì„±ê³µì„¹ì…˜": f"{success_count}/4"})
            
            return extracted_data
            
        except Exception as e:
            self.logger.log_error(f"ë‚´ìš©ì¶”ì¶œ_{node_title}", e)
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return {
                "í•µì‹¬ ë‚´ìš©": "ì¶”ì¶œ ì‹¤íŒ¨",
                "ìƒì„¸ í•µì‹¬ ë‚´ìš©": "ì¶”ì¶œ ì‹¤íŒ¨", 
                "ì£¼ìš” í™”ì œ": "ì¶”ì¶œ ì‹¤íŒ¨",
                "ë¶€ì°¨ í™”ì œ": "ì¶”ì¶œ ì‹¤íŒ¨"
            }
    
    async def update_composition_nodes(self, reference_info: Dict[str, str], 
                                     composition_files: List[str], 
                                     base_dir: Path) -> Dict[str, Dict[str, str]]:
        """
        êµ¬ì„± ë…¸ë“œ ì •ë³´ íŒŒì¼ ê°±ì‹  - ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        ê¸°ì¤€: ì „ì²´ ëŒ€ìƒìœ¼ë¡œ ì¶”ì¶œí•œ ì •ë³´(reference_info)
        ì°¸ê³ : êµ¬ì„± ë…¸ë“œì˜ ì¶”ì¶œ ì˜ì—­ ì •ë³´
        """
        update_results = {}
        
        for file_name in composition_files:
            file_path = base_dir / file_name
            if file_path.exists():
                updated_data = await self._update_single_composition_node(file_path, reference_info)
                update_results[file_name] = updated_data
            else:
                update_results[file_name] = {}
        
        return update_results
    
    async def _update_single_composition_node(self, file_path: Path, reference_info: Dict[str, str]) -> Dict[str, str]:
        """ë‹¨ì¼ êµ¬ì„± ë…¸ë“œ ê°±ì‹  - ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
        try:
            # ê¸°ì¡´ ì¶”ì¶œ ì˜ì—­ ë‚´ìš© ì½ê¸° (ê¸°ì¤€ ì •ë³´)
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            # ê¸°ì¡´ ì¶”ì¶œ ì˜ì—­ì—ì„œ í˜„ì¬ ì •ë³´ ì¶”ì¶œ
            current_extraction = self._extract_section_content(original_content, "ì¶”ì¶œ")
            
            # ê°±ì‹  ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ v2ì—ì„œ ì‚¬ìš©í•˜ë˜ í˜•ì‹)
            enhancement_content = f"""ê¸°ì¡´ ì •ë³´:
{current_extraction}

ì „ì²´ ì •ë³´ (ë³´ì™„ìš©):
í•µì‹¬ ë‚´ìš©: {reference_info.get('í•µì‹¬ ë‚´ìš©', '')}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {reference_info.get('ìƒì„¸ í•µì‹¬ ë‚´ìš©', '')}
ì£¼ìš” í™”ì œ: {reference_info.get('ì£¼ìš” í™”ì œ', '')}
ë¶€ì°¨ í™”ì œ: {reference_info.get('ë¶€ì°¨ í™”ì œ', '')}"""
            
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ ê°±ì‹  (enhancement íƒ€ì… ì‚¬ìš©)
            updated_info = await self.content_analyzer.analyze_content(
                content=enhancement_content,
                title=file_path.stem,
                context_type="enhancement"  # ê¸°ì¡´ v2ì—ì„œ ì‚¬ìš©í•˜ë˜ íƒ€ì…
            )
            
            self.logger.log_operation(f"êµ¬ì„±ë…¸ë“œê°±ì‹ _{file_path.name}", "ì™„ë£Œ")
            return updated_info
            
        except Exception as e:
            self.logger.log_error(f"êµ¬ì„±ë…¸ë“œê°±ì‹ _{file_path.name}", e)
            return {}
    
    async def update_current_node(self, current_file_path: Path, 
                                updated_composition_data: Dict[str, Dict[str, str]], 
                                original_info: Dict[str, str]) -> Dict[str, str]:
        """
        í˜„ì¬ ë…¸ë“œ ì •ë³´ íŒŒì¼ ê°±ì‹  - ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        ê¸°ì¤€: ì „ì²´ ëŒ€ìƒìœ¼ë¡œ ì¶”ì¶œí•œ ì •ë³´(original_info) 
        ì°¸ê³ : ì—…ë°ì´íŠ¸ëœ ê° êµ¬ì„± ë…¸ë“œ ì¶”ì¶œ ì˜ì—­ ì •ë³´(updated_composition_data)
        """
        try:
            # ì—…ë°ì´íŠ¸ëœ êµ¬ì„± ë…¸ë“œë“¤ì˜ ë‚´ìš©ì„ ë‹¤ì‹œ ê²°í•© (ê¸°ì¡´ v2 ë°©ì‹)
            updated_combined_content = self._combine_updated_composition_content(updated_composition_data)
            
            # ê¸°ì¡´ v2ì—ì„œ ì‚¬ìš©í•˜ë˜ synthesis_content í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            synthesis_content = f"""ì—…ë°ì´íŠ¸ëœ êµ¬ì„± ìš”ì†Œë“¤:
{updated_combined_content}

ê¸°ì¡´ ìƒìœ„ ì •ë³´:
í•µì‹¬ ë‚´ìš©: {original_info.get('í•µì‹¬ ë‚´ìš©', '')}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {original_info.get('ìƒì„¸ í•µì‹¬ ë‚´ìš©', '')}
ì£¼ìš” í™”ì œ: {original_info.get('ì£¼ìš” í™”ì œ', '')}
ë¶€ì°¨ í™”ì œ: {original_info.get('ë¶€ì°¨ í™”ì œ', '')}"""
            
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ ìµœì¢… í†µí•© ë¶„ì„ (synthesis íƒ€ì… ì‚¬ìš©)
            updated_info = await self.content_analyzer.analyze_content(
                content=synthesis_content,
                title=current_file_path.stem,
                context_type="synthesis"  # ê¸°ì¡´ v2ì—ì„œ ì‚¬ìš©í•˜ë˜ íƒ€ì…
            )
            
            self.logger.log_operation(f"í˜„ì¬ë…¸ë“œê°±ì‹ _{current_file_path.name}", "ì™„ë£Œ")
            return updated_info
            
        except Exception as e:
            self.logger.log_error(f"í˜„ì¬ë…¸ë“œê°±ì‹ _{current_file_path.name}", e)
            return {}
    
    def _extract_section_content(self, content: str, section: str) -> str:
        """íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        header_pattern = f"# {section}"
        header_start = content.find(header_pattern)
        
        if header_start == -1:
            return ""
        
        content_start = header_start + len(header_pattern)
        next_header_start = content.find("\n# ", content_start)
        
        if next_header_start == -1:
            section_content = content[content_start:].strip()
        else:
            section_content = content[content_start:next_header_start].strip()
        
        return section_content
    
    def _combine_updated_composition_content(self, updated_composition_data: Dict[str, Dict[str, str]]) -> str:
        """ì—…ë°ì´íŠ¸ëœ êµ¬ì„± ìš”ì†Œë“¤ì˜ ë‚´ìš© ê²°í•© (ê¸°ì¡´ v2 ë°©ì‹)"""
        combined = ""
        
        for file_name, data in updated_composition_data.items():
            if data:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                combined += f"## ========== {file_name} ==========\n\n"
                
                # 4ê°€ì§€ ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ ì¶”ê°€
                for section_name in ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]:
                    content = data.get(section_name, "")
                    if content and content != "ì¶”ì¶œ ì‹¤íŒ¨":
                        combined += f"### {section_name}\n{content}\n\n"
                
                combined += "=" * 100 + "\n\n"
        
        return combined


class DataSaver:
    """ê²°ê³¼ ì €ì¥ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: Path, logger: ProcessLogger):
        self.base_dir = base_dir
        self.logger = logger
    
    def save_to_extraction_section(self, file_path: Path, data: Dict[str, str]) -> bool:
        """ì¶”ì¶œ ì˜ì—­ì— ê²°ê³¼ ì €ì¥"""
        try:
            # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
            if not file_path.exists():
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                self._create_basic_node_file(file_path)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì¶”ì¶œ ì„¹ì…˜ ì°¾ê¸° ë˜ëŠ” ìƒì„±
            extraction_start = content.find("# ì¶”ì¶œ")
            if extraction_start == -1:
                # ì¶”ì¶œ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                content += "\n\n# ì¶”ì¶œ\n\n"
                extraction_start = content.find("# ì¶”ì¶œ")
            
            # ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© êµ¬ì„±
            extraction_content = self._build_extraction_content(data)
            
            # ì¶”ì¶œ ì„¹ì…˜ ì´í›„ ë‚´ìš© ì°¾ê¸°
            next_section_start = content.find("\n# ", extraction_start + 4)
            
            if next_section_start == -1:
                # ì¶”ì¶œì´ ë§ˆì§€ë§‰ ì„¹ì…˜
                new_content = content[:extraction_start] + f"# ì¶”ì¶œ\n\n{extraction_content}\n"
            else:
                # ì¶”ì¶œ ì´í›„ì— ë‹¤ë¥¸ ì„¹ì…˜ì´ ìˆìŒ
                new_content = content[:extraction_start] + f"# ì¶”ì¶œ\n\n{extraction_content}\n" + content[next_section_start:]
            
            # íŒŒì¼ì— ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            success_sections = len([v for v in data.values() if v and v != "ì¶”ì¶œ ì‹¤íŒ¨"])
            self.logger.log_operation(f"ì €ì¥ì™„ë£Œ_{file_path.name}", "ì„±ê³µ", {"ì„±ê³µì„¹ì…˜": success_sections})
            return True
            
        except Exception as e:
            self.logger.log_error(f"ì €ì¥ì‹¤íŒ¨_{file_path.name}", e)
            return False
    
    def _create_basic_node_file(self, file_path: Path):
        """ê¸°ë³¸ ë…¸ë“œ ì •ë³´ íŒŒì¼ êµ¬ì¡° ìƒì„±"""
        template = """# ì†ì„±
process_status: pending

# ì¶”ì¶œ

# ë‚´ìš©

# êµ¬ì„±
"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(template)
    
    def _build_extraction_content(self, data: Dict[str, str]) -> str:
        """ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© êµ¬ì„±"""
        content = ""
        
        # ì„¹ì…˜ ìˆœì„œ ì •ì˜
        section_order = ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]
        
        for section_name in section_order:
            if section_name in data and data[section_name] and data[section_name] != "ì¶”ì¶œ ì‹¤íŒ¨":
                content += f"## {section_name}\n{data[section_name]}\n\n"
        
        return content.strip()


class DialecticalSynthesisProcessor:
    """ì •ë°˜í•© ë°©ë²•ë¡  ë©”ì¸ ì²˜ë¦¬ í´ë˜ìŠ¤ V5 - Node í´ë˜ìŠ¤ ì˜ì¡´ì„± ì™„ì „ ì œê±°"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ê³µí†µ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.logger = ProcessLogger("dialectical_synthesis_v5", self.output_dir)
        self.content_analyzer = ContentAnalyzer(self.logger.logger)
        
        # ì „ë‹´ í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”
        self.data_loader = DataLoader(self.output_dir, self.logger)
        self.data_processor = DataProcessor(self.content_analyzer, self.logger)
        self.data_saver = DataSaver(self.output_dir, self.logger)
        self.node_grouper = NodeGrouper(self.logger)
        
        # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
        self.processing_results = {}
    
    async def process_nodes_from_json(self, json_path: str) -> Dict[str, Any]:
        """JSON íŒŒì¼ì—ì„œ ë…¸ë“œë¥¼ ë¡œë“œí•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬"""
        # JSON ë¡œë“œ
        if not self.node_grouper.load_nodes_from_json(json_path):
            self.logger.log_error("JSONë¡œë“œ", f"ì‹¤íŒ¨: {json_path}")
            return {}
        
        # ë¶€ëª¨ ë…¸ë“œë“¤ë§Œ í•„í„°ë§
        parent_nodes = self.node_grouper.filter_parent_nodes(self.node_grouper.nodes_data)
        
        return await self.process_nodes_batch(parent_nodes)
    
    async def process_nodes_batch(self, nodes_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë…¸ë“œ ë°°ì¹˜ ì²˜ë¦¬ - ê·¸ë£¹í™” ë° ì •ë ¬ëœ ìˆœì„œë¡œ"""
        batch_start = time.time()
        self.logger.log_operation("ë°°ì¹˜ì²˜ë¦¬ì‹œì‘", "ì‹œì‘", {"ì´ë…¸ë“œìˆ˜": len(nodes_data)})
        
        # 1. ë…¸ë“œ ê·¸ë£¹í™” ë° ì •ë ¬
        grouped_nodes = self.node_grouper.group_and_sort_nodes(nodes_data)
        processing_order = self.node_grouper.get_processing_order(grouped_nodes)
        
        self.logger.log_operation("ì²˜ë¦¬ìˆœì„œê²°ì •", "ì™„ë£Œ", 
                                {"ê·¸ë£¹ìˆ˜": len(grouped_nodes), "ì²˜ë¦¬ëŒ€ìƒ": len(processing_order)})
        
        # 2. ìˆœì°¨ì ìœ¼ë¡œ ê° ë…¸ë“œ ì²˜ë¦¬
        results = {}
        for i, node_data in enumerate(processing_order):
            node_title = node_data.get("title", "")
            self.logger.log_operation(f"ë…¸ë“œì²˜ë¦¬ì§„í–‰", "ì§„í–‰ì¤‘", 
                                    {"í˜„ì¬": f"{i+1}/{len(processing_order)}", "ë…¸ë“œ": node_title})
            result = await self.process_single_node_pipeline(node_data)
            results[node_title] = result
        
        batch_time = time.time() - batch_start
        success_count = sum(1 for r in results.values() if r)
        
        self.logger.log_operation("ë°°ì¹˜ì²˜ë¦¬ì™„ë£Œ", "ì™„ë£Œ", 
                                {"ì²˜ë¦¬ì‹œê°„": f"{batch_time:.2f}ì´ˆ", 
                                 "ì„±ê³µë…¸ë“œ": f"{success_count}/{len(results)}"})
        
        return results
    
    async def process_single_node_pipeline(self, node_data: Dict[str, Any]) -> bool:
        """ë‹¨ì¼ ë…¸ë“œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - ë°ì´í„° ì¤€ë¹„/ì²˜ë¦¬/ì €ì¥ ë‹¨ê³„ë³„ ë¶„ë¦¬"""
        node_start = time.time()
        node_title = node_data.get("title", "")
        level = node_data.get("level", 0)
        children_count = len(node_data.get("children_ids", []))
        
        self.logger.log_operation(f"íŒŒì´í”„ë¼ì¸ì‹œì‘_{node_title}", "ì‹œì‘", 
                                {"ë ˆë²¨": level, "ìì‹ìˆ˜": children_count})
        
        try:
            # 1. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ (ë¶ˆëŸ¬ì˜¤ê¸°)
            success = await self._data_preparation_stage(node_data)
            if not success:
                self.logger.log_operation(f"íŒŒì´í”„ë¼ì¸ì‹¤íŒ¨_{node_title}", "ì‹¤íŒ¨", {"ë‹¨ê³„": "ë°ì´í„°ì¤€ë¹„"})
                return False
            
            # 2. ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„ (ê°€ê³µ ë° ê°±ì‹ )
            success = await self._data_processing_stage(node_data)
            if not success:
                self.logger.log_operation(f"íŒŒì´í”„ë¼ì¸ì‹¤íŒ¨_{node_title}", "ì‹¤íŒ¨", {"ë‹¨ê³„": "ë°ì´í„°ì²˜ë¦¬"})
                return False
            
            # 3. ë°ì´í„° ì €ì¥ ë‹¨ê³„
            success = await self._data_storage_stage(node_data)
            
            node_time = time.time() - node_start
            self.logger.log_operation(f"íŒŒì´í”„ë¼ì¸ì™„ë£Œ_{node_title}", 
                                    "ì„±ê³µ" if success else "ì‹¤íŒ¨", 
                                    {"ì²˜ë¦¬ì‹œê°„": f"{node_time:.2f}ì´ˆ"})
            
            return success
            
        except Exception as e:
            self.logger.log_error(f"íŒŒì´í”„ë¼ì¸ì˜¤ë¥˜_{node_title}", e)
            return False
    
    async def _data_preparation_stage(self, node_data: Dict[str, Any]) -> bool:
        """ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ - ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            node_title = node_data.get("title", "")
            
            # gather_and_analyze_stageìš© ë‚´ìš© ì˜ì—­ ë°ì´í„° ë¡œë”©
            content_data = self.data_loader.load_node_content_data(node_data)
            if not content_data:
                self.logger.log_error(f"ë°ì´í„°ì¤€ë¹„_{node_title}", "ë‚´ìš© ì˜ì—­ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                return False
            
            # improve_individual_stageìš© ì¶”ì¶œ ì˜ì—­ ë°ì´í„° ë¡œë”©
            extraction_data = self.data_loader.load_node_extraction_data(node_data)
            
            # ì²˜ë¦¬ ê²°ê³¼ì— ì €ì¥
            self.processing_results[node_title] = {
                "content_data": content_data,
                "extraction_data": extraction_data,
                "node_data": node_data,
                "stage": "ì¤€ë¹„ì™„ë£Œ"
            }
            
            self.logger.log_operation(f"ë°ì´í„°ì¤€ë¹„_{node_title}", "ì„±ê³µ")
            return True
            
        except Exception as e:
            self.logger.log_error(f"ë°ì´í„°ì¤€ë¹„_{node_data.get('title', '')}", e)
            return False
    
    async def _data_processing_stage(self, node_data: Dict[str, Any]) -> bool:
        """ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„ - ê°€ê³µ ë° ê°±ì‹  (ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        try:
            node_title = node_data.get("title", "")
            node_result = self.processing_results.get(node_title, {})
            content_data = node_result.get("content_data", "")
            
            if not content_data:
                return False
            
            # ë°ì´í„° ê°€ê³µ: ê¸°ì¡´ Claude SDK í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ 4ê°€ì§€ ì •ë³´ ì¶”ì¶œ
            extracted_info = await self.data_processor.process_content_extraction(content_data, node_title)
            
            # êµ¬ì„± íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            node_file_path = self.data_loader._get_node_file_path(node_data)
            composition_files = self.data_loader._get_composition_files(node_file_path)
            
            # ë°ì´í„° ê°±ì‹ : êµ¬ì„± ë…¸ë“œë“¤ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            updated_composition_data = {}
            if composition_files:
                updated_composition_data = await self.data_processor.update_composition_nodes(
                    extracted_info, composition_files, self.output_dir
                )
                
                # í˜„ì¬ ë…¸ë“œ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
                if node_file_path.exists():
                    current_updated = await self.data_processor.update_current_node(
                        node_file_path, updated_composition_data, extracted_info
                    )
                    # í˜„ì¬ ë…¸ë“œ ê°±ì‹  ê²°ê³¼ë¥¼ extracted_infoì— ë°˜ì˜
                    if current_updated:
                        extracted_info.update(current_updated)
            
            # ê²°ê³¼ ì €ì¥
            self.processing_results[node_title].update({
                "extracted_info": extracted_info,
                "updated_composition_data": updated_composition_data,
                "stage": "ì²˜ë¦¬ì™„ë£Œ"
            })
            
            self.logger.log_operation(f"ë°ì´í„°ì²˜ë¦¬_{node_title}", "ì„±ê³µ", 
                                    {"êµ¬ì„±íŒŒì¼ìˆ˜": len(composition_files)})
            return True
            
        except Exception as e:
            self.logger.log_error(f"ë°ì´í„°ì²˜ë¦¬_{node_data.get('title', '')}", e)
            return False
    
    async def _data_storage_stage(self, node_data: Dict[str, Any]) -> bool:
        """ë°ì´í„° ì €ì¥ ë‹¨ê³„"""
        try:
            node_title = node_data.get("title", "")
            node_result = self.processing_results.get(node_title, {})
            extracted_info = node_result.get("extracted_info", {})
            updated_composition_data = node_result.get("updated_composition_data", {})
            
            if not extracted_info:
                return False
            
            # 1. í˜„ì¬ ë…¸ë“œ íŒŒì¼ì˜ ì¶”ì¶œ ì˜ì—­ì— ì €ì¥
            node_file_path = self.data_loader._get_node_file_path(node_data)
            success = self.data_saver.save_to_extraction_section(node_file_path, extracted_info)
            
            # 2. êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì˜ì—­ì—ë„ ì €ì¥
            composition_save_count = 0
            for file_name, comp_data in updated_composition_data.items():
                if comp_data:  # ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    comp_file_path = self.output_dir / file_name
                    if self.data_saver.save_to_extraction_section(comp_file_path, comp_data):
                        composition_save_count += 1
            
            if success:
                self.processing_results[node_title]["stage"] = "ì €ì¥ì™„ë£Œ"
            
            self.logger.log_operation(f"ë°ì´í„°ì €ì¥_{node_title}", "ì„±ê³µ" if success else "ì‹¤íŒ¨", 
                                    {"í˜„ì¬ë…¸ë“œ": "ì„±ê³µ" if success else "ì‹¤íŒ¨", 
                                     "êµ¬ì„±ë…¸ë“œ": f"{composition_save_count}/{len(updated_composition_data)}"})
            
            return success
            
        except Exception as e:
            self.logger.log_error(f"ë°ì´í„°ì €ì¥_{node_data.get('title', '')}", e)
            return False


async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    json_path = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-14/nodes.json"
    output_dir = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-14/node_docs"
    
    print("=" * 60)
    print("ì •ë°˜í•© ë°©ë²•ë¡  ì‹œìŠ¤í…œ V5 - Node í´ë˜ìŠ¤ ì˜ì¡´ì„± ì™„ì „ ì œê±°")
    print("=" * 60)
    
    # ì •ë°˜í•© í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
    processor = DialecticalSynthesisProcessor(output_dir)
    results = await processor.process_nodes_from_json(json_path)
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    success_count = sum(1 for success in results.values() if success)
    print(f"  - ì„±ê³µ: {success_count}/{len(results)}")
    print(f"  - ì‹¤íŒ¨: {len(results) - success_count}/{len(results)}")
    
    if results:
        print(f"\nğŸ“‹ ì²˜ë¦¬ëœ ë…¸ë“œë“¤:")
        for node_title, success in results.items():
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {node_title}")


if __name__ == "__main__":
    asyncio.run(main())