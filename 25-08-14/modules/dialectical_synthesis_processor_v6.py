"""
ìƒì„± ì‹œê°„: 2025-08-14 14:29:39 KST (ìˆ˜ì •: 2025-08-14 19:25:10 KST)
í•µì‹¬ ë‚´ìš©: ì •ë°˜í•© ë°©ë²•ë¡  V6 - DataLoader ê²½ë¡œ ìˆ˜ì • ë° ë¡œê¹… ì‹œìŠ¤í…œ V2 ì ìš©
ìƒì„¸ ë‚´ìš©:
    - DataLoader í´ë˜ìŠ¤ (ë¼ì¸ 26-): ëª©ì ë³„ ë¶„ë¦¬ (ì¶”ì¶œ/ì—…ë°ì´íŠ¸ ì „ìš©), node_docs_dir ê¸°ë°˜
    - DataProcessor í´ë˜ìŠ¤ (ë¼ì¸ 179-): 3ë‹¨ê³„ ì²˜ë¦¬ ë¡œì§ ë¶„ë¦¬
    - DataSaver í´ë˜ìŠ¤ (ë¼ì¸ 344-): ê²°ê³¼ ì €ì¥ ë° status ê´€ë¦¬
    - DialecticalSynthesisProcessor í´ë˜ìŠ¤ (ë¼ì¸ 482-): ì˜¬ë°”ë¥¸ ë…¸ë“œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    - í•µì‹¬ ìˆ˜ì •: DataLoaderê°€ node_docs_dirë¥¼ base_dirë¡œ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë…¸ë“œ íŒŒì¼ ì ‘ê·¼
    - ë¡œê¹… ì‹œìŠ¤í…œ V2 ì ìš©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì „ìš© í™•ì¥ ë¡œê¹… ì§€ì›
    - main() í•¨ìˆ˜ ë§¤ê°œë³€ìˆ˜ëª… ë³€ê²½: output_dir â†’ node_docs_dir
ìƒíƒœ: í™œì„±
ì£¼ì†Œ: dialectical_synthesis_processor_v6/fixed
ì°¸ì¡°: dialectical_synthesis_processor_v5.py, logging_system_v2.py
"""

import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any

from node_grouper import NodeGrouper
from content_analysis_module import ContentAnalyzer
from logging_system_v2 import ProcessLogger


class DataLoader:
    """ëª©ì ë³„ë¡œ ë¶„ë¦¬ëœ ë…¸ë“œ ì •ë³´ ë¡œë”© í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: Path, logger: ProcessLogger):
        self.base_dir = base_dir
        self.logger = logger
    
    def load_for_extraction(self, node_data: Dict[str, Any]) -> str:
        """ì¶”ì¶œ ì „ìš©: ë‚´ìš© ì„¹ì…˜ + êµ¬ì„± íŒŒì¼ë“¤ì˜ ë‚´ìš© ì„¹ì…˜ ê²°í•©"""
        try:
            node_title = node_data.get("title", "")
            
            # ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ
            node_file_path = self._get_node_file_path(node_data)
            if not node_file_path.exists():
                self.logger.log_error(f"ë…¸ë“œíŒŒì¼ì—†ìŒ_{node_title}", f"íŒŒì¼ ì—†ìŒ: {node_file_path}")
                return ""
            
            # 1. í˜„ì¬ ë…¸ë“œì˜ ë‚´ìš© ì„¹ì…˜ ì¶”ì¶œ
            current_content = self._extract_section_from_file(node_file_path, "ë‚´ìš©")
            
            # 2. êµ¬ì„± íŒŒì¼ë“¤ì˜ ë‚´ìš© ì„¹ì…˜ ìˆ˜ì§‘
            composition_files = self._get_composition_files(node_file_path)
            composition_contents = []
            
            for file_name in composition_files:
                comp_file_path = self.base_dir / file_name
                if comp_file_path.exists():
                    comp_content = self._extract_section_from_file(comp_file_path, "ë‚´ìš©")
                    if comp_content.strip():
                        composition_contents.append(f"=== {file_name} ë‚´ìš© ===\n{comp_content}")
            
            # 3. í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
            combined_content = ""
            
            # í˜„ì¬ ë…¸ë“œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if current_content.strip():
                combined_content += f"# {node_title} - í˜„ì¬ ë…¸ë“œ ë‚´ìš©\n{current_content}\n\n"
            
            # êµ¬ì„± íŒŒì¼ë“¤ ë‚´ìš© ì¶”ê°€
            if composition_contents:
                combined_content += f"# {node_title} - êµ¬ì„± ìš”ì†Œë“¤ ë‚´ìš©\n\n"
                combined_content += "\n\n".join(composition_contents)
            
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
            if not combined_content.strip():
                self.logger.log_operation(f"ì¶”ì¶œë°ì´í„°ë¡œë”©_{node_title}", "ê²½ê³ ", 
                                        {"íƒ€ì…": "ë‚´ìš©ì—†ìŒ", "í˜„ì¬ë‚´ìš©": len(current_content), 
                                         "êµ¬ì„±íŒŒì¼ìˆ˜": len(composition_files)})
                return ""
            
            self.logger.log_operation(f"ì¶”ì¶œë°ì´í„°ë¡œë”©_{node_title}", "ì„±ê³µ", 
                                    {"íƒ€ì…": "ë‚´ìš©+êµ¬ì„±ê²°í•©", "í˜„ì¬ë‚´ìš©": len(current_content),
                                     "êµ¬ì„±íŒŒì¼ìˆ˜": len(composition_files), "ì´í…ìŠ¤íŠ¸ê¸¸ì´": len(combined_content)})
            return combined_content
            
        except Exception as e:
            self.logger.log_error(f"ì¶”ì¶œë°ì´í„°ë¡œë”©_{node_data.get('title', '')}", e)
            return ""
    
    def load_for_update(self, node_data: Dict[str, Any]) -> str:
        """ì—…ë°ì´íŠ¸ ì „ìš©: ë‚´ìš© + ìì‹ë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ ê²°í•©"""
        try:
            node_title = node_data.get("title", "")
            
            # ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ
            node_file_path = self._get_node_file_path(node_data)
            if not node_file_path.exists():
                self.logger.log_error(f"ë…¸ë“œíŒŒì¼ì—†ìŒ_{node_title}", f"íŒŒì¼ ì—†ìŒ: {node_file_path}")
                return ""
            
            # 1. í˜„ì¬ ë…¸ë“œì˜ ë‚´ìš© ì„¹ì…˜
            current_content = self._extract_section_from_file(node_file_path, "ë‚´ìš©")
            
            # 2. ìì‹ ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ ìˆ˜ì§‘
            composition_extractions = self.load_composition_extractions(node_data)
            
            # 3. ê²°í•©
            combined_text = self._combine_for_update(current_content, composition_extractions, node_title)
            
            self.logger.log_operation(f"ì—…ë°ì´íŠ¸ë°ì´í„°ë¡œë”©_{node_title}", "ì„±ê³µ", 
                                    {"íƒ€ì…": "ë‚´ìš©+ìì‹ì¶”ì¶œ", "ìì‹ìˆ˜": len(composition_extractions), 
                                     "í…ìŠ¤íŠ¸ê¸¸ì´": len(combined_text)})
            return combined_text
            
        except Exception as e:
            self.logger.log_error(f"ì—…ë°ì´íŠ¸ë°ì´í„°ë¡œë”©_{node_data.get('title', '')}", e)
            return ""
    
    def load_composition_extractions(self, node_data: Dict[str, Any]) -> List[str]:
        """êµ¬ì„± íŒŒì¼ë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ë§Œ ìˆ˜ì§‘"""
        try:
            node_file_path = self._get_node_file_path(node_data)
            composition_files = self._get_composition_files(node_file_path)
            
            extractions = []
            for file_name in composition_files:
                file_path = self.base_dir / file_name
                if file_path.exists():
                    extraction = self._extract_section_from_file(file_path, "ì¶”ì¶œ")
                    if extraction:
                        extractions.append(f"=== {file_name} ===\n{extraction}")
            
            return extractions
            
        except Exception as e:
            self.logger.log_error(f"êµ¬ì„±ì¶”ì¶œë¡œë”©_{node_data.get('title', '')}", e)
            return []
    
    def get_composition_files(self, node_data: Dict[str, Any]) -> List[str]:
        """êµ¬ì„± ì„¹ì…˜ì—ì„œ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ"""
        try:
            node_file_path = self._get_node_file_path(node_data)
            return self._get_composition_files(node_file_path)
        except Exception:
            return []
    
    def _get_node_file_path(self, node_data: Dict[str, Any]) -> Path:
        """ë…¸ë“œ ì •ë³´ íŒŒì¼ ê²½ë¡œ ìƒì„± - ì‹¤ì œ íŒŒì¼ëª… ê·œì¹™ì— ë§ì¶¤"""
        node_id = node_data.get("id", 0)
        level = node_data.get("level", 0)
        title = node_data.get("title", "")
        # ì‹¤ì œ íŒŒì¼ëª…: ì†Œë¬¸ì, ê³µë°±ì€ ì–¸ë”ìŠ¤ì½”ì–´, í•˜ì´í”ˆ ìœ ì§€
        safe_title = title.lower().replace(" ", "_").replace("/", "_").replace("\\", "_")
        safe_title = safe_title.replace("-", "_")  # í•˜ì´í”ˆë„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        return self.base_dir / f"{node_id:02d}_lev{level}_{safe_title}_info.md"
    
    def _extract_section_from_file(self, file_path: Path, section_name: str) -> str:
        """íŒŒì¼ì—ì„œ íŠ¹ì • ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ - í•˜ìœ„ í—¤ë” í¬í•¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            header_pattern = f"# {section_name}"
            header_start = content.find(header_pattern)
            
            if header_start == -1:
                return ""
            
            content_start = header_start + len(header_pattern)
            
            # ë‹¤ìŒ ë©”ì¸ ì„¹ì…˜ í—¤ë”ë¥¼ ì°¾ê¸° (ê°™ì€ ë ˆë²¨ì˜ í—¤ë”ë§Œ)
            # ì¤„ ì‹œì‘ì—ì„œ ì‹œì‘í•˜ëŠ” "# " íŒ¨í„´ì„ ì°¾ì•„ì•¼ í•¨
            lines = content[content_start:].split('\n')
            section_lines = []
            
            for line in lines:
                # ì¤„ ì‹œì‘ì—ì„œ "# "ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì€ ë©”ì¸ ì„¹ì…˜ í—¤ë”
                if line.strip().startswith('# ') and not line.strip().startswith('##'):
                    break
                section_lines.append(line)
            
            section_content = '\n'.join(section_lines).strip()
            return section_content
            
        except Exception as e:
            self.logger.log_error(f"ì„¹ì…˜ì¶”ì¶œ_{file_path.name}", e)
            return ""
    
    def _get_composition_files(self, node_file_path: Path) -> List[str]:
        """êµ¬ì„± ì„¹ì…˜ì—ì„œ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ"""
        try:
            composition_content = self._extract_section_from_file(node_file_path, "êµ¬ì„±")
            if not composition_content:
                return []
            
            files = []
            for line in composition_content.split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and line.endswith('.md'):
                    files.append(line)
            
            return files
            
        except Exception:
            return []
    
    def _combine_for_update(self, current_content: str, composition_extractions: List[str], node_title: str) -> str:
        """ì—…ë°ì´íŠ¸ìš© ë°ì´í„° ê²°í•©"""
        combined = f"# {node_title} - ì—…ë°ì´íŠ¸ìš© í†µí•©\n\n"
        
        if current_content:
            combined += f"## í˜„ì¬ ë…¸ë“œ ë‚´ìš©\n{current_content}\n\n"
        
        if composition_extractions:
            combined += "## ìì‹ ë…¸ë“œë“¤ ì¶”ì¶œ ì •ë³´\n\n"
            for extraction in composition_extractions:
                combined += f"{extraction}\n\n"
        
        return combined


class DataProcessor:
    """3ë‹¨ê³„ ì²˜ë¦¬ ë¡œì§ì„ ë¶„ë¦¬í•œ ë°ì´í„° ê°€ê³µ í´ë˜ìŠ¤"""
    
    def __init__(self, content_analyzer: ContentAnalyzer, logger: ProcessLogger):
        self.content_analyzer = content_analyzer
        self.logger = logger
        # ë³‘ë ¬ì²˜ë¦¬ ì œí•œ: ìµœëŒ€ 2ê°œ ë™ì‹œ ì‹¤í–‰ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        self.semaphore = asyncio.Semaphore(2)
        self._active_tasks = set()  # í™œì„± íƒœìŠ¤í¬ ì¶”ì 
    
    async def process_content_extraction(self, content: str, node_title: str) -> Dict[str, str]:
        """ìˆœìˆ˜ ì¶”ì¶œ ì‘ì—…: 4ê°€ì§€ ì •ë³´ ì¶”ì¶œ (ë¦¬í”„/ë¶€ëª¨ ê³µí†µ) - ìì› ê´€ë¦¬ í¬í•¨"""
        async with self.semaphore:  # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
            process_start = time.time()
            task_id = f"extract_{node_title}_{id(asyncio.current_task())}"
            
            try:
                # í™œì„± íƒœìŠ¤í¬ ë“±ë¡
                self._active_tasks.add(task_id)
                self.logger.log_operation(f"ë‚´ìš©ì¶”ì¶œì‹œì‘_{node_title}", "ì‹œì‘", 
                                        {"í™œì„±íƒœìŠ¤í¬ìˆ˜": len(self._active_tasks)})
                
                result = await self.content_analyzer.analyze_content(
                    content=content,
                    title=node_title,
                    context_type="combined"
                )
                
                extracted_data = {}
                section_names = ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]
                
                for section_name in section_names:
                    content_result = result.get(section_name, "")
                    if content_result and not content_result.startswith("ë¶„ì„ ì‹¤íŒ¨") and len(content_result.strip()) > 0:
                        extracted_data[section_name] = content_result
                    else:
                        extracted_data[section_name] = "ì¶”ì¶œ ì‹¤íŒ¨"
                
                process_time = time.time() - process_start
                success_count = len([v for v in extracted_data.values() if v != "ì¶”ì¶œ ì‹¤íŒ¨"])
                
                self.logger.log_operation(f"ë‚´ìš©ì¶”ì¶œì™„ë£Œ_{node_title}", "ì™„ë£Œ", 
                                        {"ì²˜ë¦¬ì‹œê°„": f"{process_time:.2f}ì´ˆ", "ì„±ê³µì„¹ì…˜": f"{success_count}/4"})
                
                return extracted_data
                
            except Exception as e:
                self.logger.log_error(f"ë‚´ìš©ì¶”ì¶œ_{node_title}", e)
                return {
                    "í•µì‹¬ ë‚´ìš©": "ì¶”ì¶œ ì‹¤íŒ¨",
                    "ìƒì„¸ í•µì‹¬ ë‚´ìš©": "ì¶”ì¶œ ì‹¤íŒ¨", 
                    "ì£¼ìš” í™”ì œ": "ì¶”ì¶œ ì‹¤íŒ¨",
                    "ë¶€ì°¨ í™”ì œ": "ì¶”ì¶œ ì‹¤íŒ¨"
                }
            finally:
                # ìì› ì •ë¦¬: íƒœìŠ¤í¬ ì œê±°
                self._active_tasks.discard(task_id)
                remaining_tasks = len(self._active_tasks)
                if remaining_tasks > 0:
                    self.logger.log_operation(f"ìì›ì •ë¦¬_{node_title}", "ì™„ë£Œ", 
                                            {"ë‚¨ì€íƒœìŠ¤í¬ìˆ˜": remaining_tasks})
    
    async def update_composition_nodes(self, reference_info: Dict[str, str], 
                                     composition_files: List[str], 
                                     base_dir: Path) -> Dict[str, Dict[str, str]]:
        """ìì‹ ë…¸ë“œë“¤ ì—…ë°ì´íŠ¸ (ë¶€ëª¨ ì¶”ì¶œ ì •ë³´ ê¸°ë°˜)"""
        update_results = {}
        
        for file_name in composition_files:
            file_path = base_dir / file_name
            if file_path.exists():
                updated_data = await self._update_single_composition_node(file_path, reference_info)
                update_results[file_name] = updated_data
            else:
                update_results[file_name] = {}
        
        return update_results
    
    async def process_synthesis_update(self, combined_content: str, 
                                     updated_children: Dict[str, Dict[str, str]], 
                                     original_info: Dict[str, str],
                                     node_title: str) -> Dict[str, str]:
        """ìµœì¢… í†µí•© ì—…ë°ì´íŠ¸ (ì—…ë°ì´íŠ¸ëœ ìì‹ ì •ë³´ ë°˜ì˜)"""
        try:
            # ì—…ë°ì´íŠ¸ëœ ìì‹ ë…¸ë“œë“¤ì˜ ë‚´ìš©ì„ ê²°í•©
            updated_combined_content = self._combine_updated_composition_content(updated_children)
            
            # í†µí•© ë¶„ì„ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            synthesis_content = f"""ì—…ë°ì´íŠ¸ëœ ìì‹ ìš”ì†Œë“¤:
{updated_combined_content}

í˜„ì¬ ë…¸ë“œ ë‚´ìš©ê³¼ ê¸°ì¡´ ì¶”ì¶œ ì •ë³´:
{combined_content}

ê¸°ì¡´ ìƒìœ„ ì •ë³´:
í•µì‹¬ ë‚´ìš©: {original_info.get('í•µì‹¬ ë‚´ìš©', '')}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {original_info.get('ìƒì„¸ í•µì‹¬ ë‚´ìš©', '')}
ì£¼ìš” í™”ì œ: {original_info.get('ì£¼ìš” í™”ì œ', '')}
ë¶€ì°¨ í™”ì œ: {original_info.get('ë¶€ì°¨ í™”ì œ', '')}"""
            
            # ìµœì¢… í†µí•© ë¶„ì„
            updated_info = await self.content_analyzer.analyze_content(
                content=synthesis_content,
                title=node_title,
                context_type="synthesis"
            )
            
            self.logger.log_operation(f"í†µí•©ì—…ë°ì´íŠ¸_{node_title}", "ì™„ë£Œ")
            return updated_info
            
        except Exception as e:
            self.logger.log_error(f"í†µí•©ì—…ë°ì´íŠ¸_{node_title}", e)
            return {}
    
    async def _update_single_composition_node(self, file_path: Path, reference_info: Dict[str, str]) -> Dict[str, str]:
        """ë‹¨ì¼ ìì‹ ë…¸ë“œ ê°±ì‹ """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            current_extraction = self._extract_section_content(original_content, "ì¶”ì¶œ")
            
            enhancement_content = f"""ê¸°ì¡´ ì •ë³´:
{current_extraction}

ì „ì²´ ì •ë³´ (ë³´ì™„ìš©):
í•µì‹¬ ë‚´ìš©: {reference_info.get('í•µì‹¬ ë‚´ìš©', '')}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {reference_info.get('ìƒì„¸ í•µì‹¬ ë‚´ìš©', '')}
ì£¼ìš” í™”ì œ: {reference_info.get('ì£¼ìš” í™”ì œ', '')}
ë¶€ì°¨ í™”ì œ: {reference_info.get('ë¶€ì°¨ í™”ì œ', '')}"""
            
            updated_info = await self.content_analyzer.analyze_content(
                content=enhancement_content,
                title=file_path.stem,
                context_type="enhancement"
            )
            
            self.logger.log_operation(f"ìì‹ë…¸ë“œê°±ì‹ _{file_path.name}", "ì™„ë£Œ")
            return updated_info
            
        except Exception as e:
            self.logger.log_error(f"ìì‹ë…¸ë“œê°±ì‹ _{file_path.name}", e)
            return {}
    
    def _extract_section_content(self, content: str, section: str) -> str:
        """íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        header_pattern = f"# {section}"
        header_start = content.find(header_pattern)
        
        if header_start == -1:
            return ""
        
        content_start = header_start + len(header_pattern)
        next_header_start = content.find("\n# ", content_start)
        
        if next_header_start == -1:
            section_content = content[content_start:].strip()
        else:
            section_content = content[content_start:next_header_start].strip()
        
        return section_content
    
    def _combine_updated_composition_content(self, updated_composition_data: Dict[str, Dict[str, str]]) -> str:
        """ì—…ë°ì´íŠ¸ëœ ìì‹ ìš”ì†Œë“¤ì˜ ë‚´ìš© ê²°í•©"""
        combined = ""
        
        for file_name, data in updated_composition_data.items():
            if data:
                combined += f"## ========== {file_name} ==========\n\n"
                
                for section_name in ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]:
                    content = data.get(section_name, "")
                    if content and content != "ì¶”ì¶œ ì‹¤íŒ¨":
                        combined += f"### {section_name}\n{content}\n\n"
                
                combined += "=" * 100 + "\n\n"
        
        return combined


class DataSaver:
    """ê²°ê³¼ ì €ì¥ ë° status ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: Path, logger: ProcessLogger):
        self.base_dir = base_dir
        self.logger = logger
    
    def save_to_extraction_section(self, file_path: Path, data: Dict[str, str]) -> bool:
        """ì¶”ì¶œ ì˜ì—­ì— ê²°ê³¼ ì €ì¥"""
        try:
            if not file_path.exists():
                self._create_basic_node_file(file_path)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            extraction_start = content.find("# ì¶”ì¶œ")
            if extraction_start == -1:
                content += "\n\n# ì¶”ì¶œ\n\n"
                extraction_start = content.find("# ì¶”ì¶œ")
            
            extraction_content = self._build_extraction_content(data)
            
            next_section_start = content.find("\n# ", extraction_start + 4)
            
            if next_section_start == -1:
                new_content = content[:extraction_start] + f"# ì¶”ì¶œ\n\n{extraction_content}\n"
            else:
                new_content = content[:extraction_start] + f"# ì¶”ì¶œ\n\n{extraction_content}\n" + content[next_section_start:]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            success_sections = len([v for v in data.values() if v and v != "ì¶”ì¶œ ì‹¤íŒ¨"])
            self.logger.log_operation(f"ì €ì¥ì™„ë£Œ_{file_path.name}", "ì„±ê³µ", {"ì„±ê³µì„¹ì…˜": success_sections})
            return True
            
        except Exception as e:
            self.logger.log_error(f"ì €ì¥ì‹¤íŒ¨_{file_path.name}", e)
            return False
    
    def update_node_status(self, file_path: Path, status: bool) -> bool:
        """ë…¸ë“œ ì •ë³´ íŒŒì¼ì˜ process_status í•„ë“œ ì—…ë°ì´íŠ¸"""
        try:
            if not file_path.exists():
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì†ì„± ì„¹ì…˜ì—ì„œ process_status ì°¾ê¸°/ì—…ë°ì´íŠ¸
            status_pattern = "process_status:"
            status_start = content.find(status_pattern)
            
            if status_start == -1:
                # process_statusê°€ ì—†ìœ¼ë©´ ì†ì„± ì„¹ì…˜ì— ì¶”ê°€
                attr_start = content.find("# ì†ì„±")
                if attr_start != -1:
                    attr_end = content.find("\n# ", attr_start + 4)
                    if attr_end == -1:
                        # ì†ì„±ì´ ë§ˆì§€ë§‰ ì„¹ì…˜
                        new_content = content + f"\nprocess_status: {str(status).lower()}\n"
                    else:
                        # ì†ì„± ì„¹ì…˜ ëì— ì¶”ê°€
                        new_content = content[:attr_end] + f"\nprocess_status: {str(status).lower()}\n" + content[attr_end:]
                else:
                    # ì†ì„± ì„¹ì…˜ì´ ì—†ìœ¼ë©´ íŒŒì¼ ì‹œì‘ì— ì¶”ê°€
                    new_content = f"# ì†ì„±\nprocess_status: {str(status).lower()}\n\n{content}"
            else:
                # ê¸°ì¡´ process_status ê°’ ì—…ë°ì´íŠ¸
                line_end = content.find('\n', status_start)
                if line_end == -1:
                    line_end = len(content)
                
                new_content = content[:status_start] + f"process_status: {str(status).lower()}" + content[line_end:]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            self.logger.log_operation(f"ìƒíƒœì—…ë°ì´íŠ¸_{file_path.name}", "ì„±ê³µ", {"status": str(status).lower()})
            return True
            
        except Exception as e:
            self.logger.log_error(f"ìƒíƒœì—…ë°ì´íŠ¸_{file_path.name}", e)
            return False
    
    def check_node_status(self, file_path: Path) -> bool:
        """ë…¸ë“œì˜ process_status í™•ì¸"""
        try:
            if not file_path.exists():
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            status_pattern = "process_status:"
            status_start = content.find(status_pattern)
            
            if status_start == -1:
                return False
            
            line_end = content.find('\n', status_start)
            if line_end == -1:
                line_end = len(content)
            
            status_line = content[status_start:line_end]
            return "true" in status_line.lower()
            
        except Exception:
            return False
    
    def _create_basic_node_file(self, file_path: Path):
        """ê¸°ë³¸ ë…¸ë“œ ì •ë³´ íŒŒì¼ êµ¬ì¡° ìƒì„±"""
        template = """# ì†ì„±
process_status: false

# ì¶”ì¶œ

# ë‚´ìš©

# êµ¬ì„±
"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(template)
    
    def _build_extraction_content(self, data: Dict[str, str]) -> str:
        """ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© êµ¬ì„±"""
        content = ""
        
        section_order = ["í•µì‹¬ ë‚´ìš©", "ìƒì„¸ í•µì‹¬ ë‚´ìš©", "ì£¼ìš” í™”ì œ", "ë¶€ì°¨ í™”ì œ"]
        
        for section_name in section_order:
            if section_name in data and data[section_name] and data[section_name] != "ì¶”ì¶œ ì‹¤íŒ¨":
                content += f"## {section_name}\n{data[section_name]}\n\n"
        
        return content.strip()


class DialecticalSynthesisProcessor:
    """ì •ë°˜í•© ë°©ë²•ë¡  ë©”ì¸ ì²˜ë¦¬ í´ë˜ìŠ¤ V6 - ì˜ì¡´ì„± ê¸°ë°˜ ë ˆë²¨ë³„ ì²˜ë¦¬"""
    
    def __init__(self, node_docs_dir: str):
        self.node_docs_dir = Path(node_docs_dir)  # ì‹¤ì œ ë…¸ë“œ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        self.node_docs_dir.mkdir(exist_ok=True)
        
        # ë¡œê·¸ ì¶œë ¥ìš© ë””ë ‰í† ë¦¬ (node_docs_dirì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬)
        self.output_dir = self.node_docs_dir.parent
        
        # ê³µí†µ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.logger = ProcessLogger("dialectical_synthesis_v6", self.output_dir)
        self.content_analyzer = ContentAnalyzer(self.logger.logger)
        
        # ì „ë‹´ í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” - DataLoaderëŠ” ì‹¤ì œ ë…¸ë“œ íŒŒì¼ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        self.data_loader = DataLoader(self.node_docs_dir, self.logger)
        self.data_processor = DataProcessor(self.content_analyzer, self.logger)
        self.data_saver = DataSaver(self.node_docs_dir, self.logger)
        self.node_grouper = NodeGrouper(self.logger)
        
        # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
        self.processing_results = {}
    
    async def process_nodes_from_json(self, json_path: str) -> Dict[str, Any]:
        """JSON íŒŒì¼ì—ì„œ ë…¸ë“œë¥¼ ë¡œë“œí•˜ì—¬ ì˜ì¡´ì„± ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬"""
        # JSON ë¡œë“œ
        if not self.node_grouper.load_nodes_from_json(json_path):
            self.logger.log_error("JSONë¡œë“œ", f"ì‹¤íŒ¨: {json_path}")
            return {}
        
        return await self.process_nodes_with_dependencies(self.node_grouper.nodes_data)
    
    async def process_nodes_with_dependencies(self, nodes_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ê¸°ë°˜ ë…¸ë“œ ì²˜ë¦¬ - ë ˆë²¨ë³„ ìˆœì°¨, ë ˆë²¨ ë‚´ ì˜ì¡´ì„± ê³ ë ¤"""
        batch_start = time.time()
        self.logger.log_operation("ì˜ì¡´ì„±ì²˜ë¦¬ì‹œì‘", "ì‹œì‘", {"ì´ë…¸ë“œìˆ˜": len(nodes_data)})
        
        # 1. ë ˆë²¨ë³„ ë…¸ë“œ ë¶„ë¥˜ (ë¦¬í”„/ë¶€ëª¨ ë¶„ë¦¬)
        level_categories = self.categorize_nodes_by_level(nodes_data)
        
        # 2. ë†’ì€ ë ˆë²¨(í•˜ìœ„)ë¶€í„° ìˆœì°¨ ì²˜ë¦¬
        total_results = {}
        for level in sorted(level_categories.keys(), reverse=True):
            category = level_categories[level]
            
            self.logger.log_operation(f"ë ˆë²¨{level}ì²˜ë¦¬ì‹œì‘", "ì‹œì‘", 
                                    {"ë¦¬í”„ë…¸ë“œ": len(category["leaf_nodes"]), 
                                     "ë¶€ëª¨ë…¸ë“œ": len(category["parent_nodes"])})
            
            # ë ˆë²¨ë³„ ì²˜ë¦¬
            level_success = await self.process_level_with_dependencies(level, category)
            
            if not level_success:
                self.logger.log_error(f"ë ˆë²¨{level}ì²˜ë¦¬", "ì‹¤íŒ¨ - ìƒìœ„ ë ˆë²¨ ì²˜ë¦¬ ì¤‘ë‹¨")
                break
            
            # ê²°ê³¼ ìˆ˜ì§‘
            total_results.update(self.collect_level_results(category))
        
        batch_time = time.time() - batch_start
        success_count = sum(1 for r in total_results.values() if r)
        
        self.logger.log_operation("ì˜ì¡´ì„±ì²˜ë¦¬ì™„ë£Œ", "ì™„ë£Œ", 
                                {"ì²˜ë¦¬ì‹œê°„": f"{batch_time:.2f}ì´ˆ", 
                                 "ì„±ê³µë…¸ë“œ": f"{success_count}/{len(total_results)}"})
        
        return total_results
    
    def categorize_nodes_by_level(self, nodes_data: List[Dict[str, Any]]) -> Dict[int, Dict[str, List[Dict[str, Any]]]]:
        """ë ˆë²¨ë³„ë¡œ ë…¸ë“œë¥¼ ë¦¬í”„/ë¶€ëª¨ë¡œ ë¶„ë¥˜"""
        level_categories = {}
        
        for node in nodes_data:
            level = node.get("level", 0)
            if level not in level_categories:
                level_categories[level] = {"leaf_nodes": [], "parent_nodes": []}
            
            children_ids = node.get("children_ids", [])
            if children_ids and len(children_ids) > 0:
                level_categories[level]["parent_nodes"].append(node)
            else:
                level_categories[level]["leaf_nodes"].append(node)
        
        return level_categories
    
    async def process_level_with_dependencies(self, level: int, category: Dict[str, List[Dict[str, Any]]]) -> bool:
        """ë ˆë²¨ ë‚´ ì˜ì¡´ì„± ê¸°ë°˜ ì²˜ë¦¬"""
        
        # 1ë‹¨ê³„: ëª¨ë“  ë¦¬í”„ ë…¸ë“œ ì²˜ë¦¬ (ë³‘ë ¬ ê°€ëŠ¥)
        if category["leaf_nodes"]:
            leaf_results = await self.process_all_leaf_nodes(category["leaf_nodes"])
            
            # ë¦¬í”„ ë…¸ë“œ ì™„ë£Œ ê²€ì¦
            if not self.verify_nodes_completion(category["leaf_nodes"]):
                self.logger.log_error(f"ë ˆë²¨{level}_ë¦¬í”„ë…¸ë“œ", "ì™„ë£Œ ê²€ì¦ ì‹¤íŒ¨")
                return False
        
        # 2ë‹¨ê³„: ë¶€ëª¨ ë…¸ë“œë“¤ ì²˜ë¦¬ (ì´ì œ ìì‹ë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ ì¡´ì¬)
        if category["parent_nodes"]:
            parent_results = await self.process_all_parent_nodes(category["parent_nodes"])
            
            # ë¶€ëª¨ ë…¸ë“œ ì™„ë£Œ ê²€ì¦
            if not self.verify_nodes_completion(category["parent_nodes"]):
                self.logger.log_error(f"ë ˆë²¨{level}_ë¶€ëª¨ë…¸ë“œ", "ì™„ë£Œ ê²€ì¦ ì‹¤íŒ¨")
                return False
        
        self.logger.log_operation(f"ë ˆë²¨{level}ì²˜ë¦¬ì™„ë£Œ", "ì„±ê³µ")
        return True
    
    async def process_all_leaf_nodes(self, leaf_nodes: List[Dict[str, Any]]) -> Dict[str, bool]:
        """ëª¨ë“  ë¦¬í”„ ë…¸ë“œ ì²˜ë¦¬ (ì œí•œëœ ë³‘ë ¬) - ìì› ê´€ë¦¬ ê°•í™”"""
        results = {}
        total_nodes = len(leaf_nodes)
        
        self.logger.log_operation("ë¦¬í”„ë…¸ë“œë³‘ë ¬ì²˜ë¦¬ì‹œì‘", "ì‹œì‘", 
                                {"ì´ë…¸ë“œìˆ˜": total_nodes, "ìµœëŒ€ë™ì‹œì‹¤í–‰": 2})
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬ (ìµœëŒ€ 2ê°œì”©)
        batch_size = 2
        for i in range(0, total_nodes, batch_size):
            batch = leaf_nodes[i:i + batch_size]
            batch_start = time.time()
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            tasks = []
            for node_data in batch:
                task = self.process_leaf_node_pipeline(node_data)
                tasks.append(task)
            
            try:
                # ë°°ì¹˜ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
                task_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for j, result in enumerate(task_results):
                    node_title = batch[j].get("title", "")
                    if isinstance(result, Exception):
                        self.logger.log_error(f"ë¦¬í”„ë…¸ë“œë³‘ë ¬_{node_title}", result)
                        results[node_title] = False
                    else:
                        results[node_title] = result
                
                batch_time = time.time() - batch_start
                batch_num = (i // batch_size) + 1
                self.logger.log_operation(f"ë°°ì¹˜{batch_num}ì™„ë£Œ", "ì™„ë£Œ", 
                                        {"ì†Œìš”ì‹œê°„": f"{batch_time:.2f}ì´ˆ", "ë…¸ë“œìˆ˜": len(batch)})
                
                # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
                if i + batch_size < total_nodes:
                    await asyncio.sleep(0.5)
                    
            except Exception as e:
                self.logger.log_error(f"ë°°ì¹˜{(i // batch_size) + 1}ì²˜ë¦¬", e)
                # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë…¸ë“œë“¤ ì‹¤íŒ¨ ì²˜ë¦¬
                for node_data in batch:
                    results[node_data.get("title", "")] = False
        
        success_count = sum(1 for success in results.values() if success)
        self.logger.log_operation("ë¦¬í”„ë…¸ë“œë³‘ë ¬ì²˜ë¦¬ì™„ë£Œ", "ì™„ë£Œ", 
                                {"ì„±ê³µ": f"{success_count}/{total_nodes}"})
        
        return results
    
    async def process_all_parent_nodes(self, parent_nodes: List[Dict[str, Any]]) -> Dict[str, bool]:
        """ëª¨ë“  ë¶€ëª¨ ë…¸ë“œ ì²˜ë¦¬ (ìˆœì°¨ - ìì‹ ì˜ì¡´ì„± ë•Œë¬¸)"""
        results = {}
        
        for node_data in parent_nodes:
            node_title = node_data.get("title", "")
            result = await self.process_parent_node_pipeline(node_data)
            results[node_title] = result
        
        return results
    
    async def process_leaf_node_pipeline(self, node_data: Dict[str, Any]) -> bool:
        """ë¦¬í”„ ë…¸ë“œ íŒŒì´í”„ë¼ì¸: ì¶”ì¶œ â†’ ì €ì¥ â†’ status ì—…ë°ì´íŠ¸"""
        node_start = time.time()
        node_title = node_data.get("title", "")
        
        self.logger.log_operation(f"ë¦¬í”„íŒŒì´í”„ë¼ì¸ì‹œì‘_{node_title}", "ì‹œì‘")
        
        try:
            # 1. ì¶”ì¶œ ì‘ì—… (ë‚´ìš© ì„¹ì…˜ë§Œ)
            extraction_data = self.data_loader.load_for_extraction(node_data)
            if not extraction_data:
                return False
            
            extracted_info = await self.data_processor.process_content_extraction(extraction_data, node_title)
            
            # 2. ì €ì¥
            node_file_path = self.data_loader._get_node_file_path(node_data)
            save_success = self.data_saver.save_to_extraction_section(node_file_path, extracted_info)
            
            # 3. status ì—…ë°ì´íŠ¸
            if save_success:
                status_success = self.data_saver.update_node_status(node_file_path, True)
                
                node_time = time.time() - node_start
                self.logger.log_operation(f"ë¦¬í”„íŒŒì´í”„ë¼ì¸ì™„ë£Œ_{node_title}", 
                                        "ì„±ê³µ" if status_success else "ì €ì¥ì„±ê³µ_ìƒíƒœì‹¤íŒ¨", 
                                        {"ì²˜ë¦¬ì‹œê°„": f"{node_time:.2f}ì´ˆ"})
                return status_success
            
            return False
            
        except Exception as e:
            self.logger.log_error(f"ë¦¬í”„íŒŒì´í”„ë¼ì¸ì˜¤ë¥˜_{node_title}", e)
            return False
    
    async def process_parent_node_pipeline(self, node_data: Dict[str, Any]) -> bool:
        """ë¶€ëª¨ ë…¸ë“œ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸"""
        node_start = time.time()
        node_title = node_data.get("title", "")
        
        self.logger.log_operation(f"ë¶€ëª¨íŒŒì´í”„ë¼ì¸ì‹œì‘_{node_title}", "ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ë¶€ëª¨ ë…¸ë“œ ì¶”ì¶œ (ë‚´ìš© ì„¹ì…˜ë§Œ) - ë¹„ì–´ìˆì–´ë„ ì§„í–‰
            extraction_data = self.data_loader.load_for_extraction(node_data)
            
            if extraction_data.strip():
                # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ì¶œ ìˆ˜í–‰
                parent_extracted = await self.data_processor.process_content_extraction(extraction_data, node_title)
            else:
                # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¹ˆ ì¶”ì¶œ ì •ë³´ë¡œ ì§„í–‰ (ìì‹ ë…¸ë“œ ì¢…í•©ìš© ë¶€ëª¨ ë…¸ë“œ)
                self.logger.log_operation(f"ë¶€ëª¨ë…¸ë“œë¹ˆë‚´ìš©_{node_title}", "ì§„í–‰", 
                                        {"ìì‹ë…¸ë“œì¢…í•©": "ì˜ˆì •"})
                parent_extracted = {
                    "í•µì‹¬ ë‚´ìš©": "",
                    "ìƒì„¸ í•µì‹¬ ë‚´ìš©": "",
                    "ì£¼ìš” í™”ì œ": "",
                    "ë¶€ì°¨ í™”ì œ": ""
                }
            
            # 2ë‹¨ê³„: ìì‹ ë…¸ë“œë“¤ ì—…ë°ì´íŠ¸ (ë¶€ëª¨ ì¶”ì¶œ ì •ë³´ ê¸°ë°˜)
            composition_files = self.data_loader.get_composition_files(node_data)
            updated_children = await self.data_processor.update_composition_nodes(
                parent_extracted, composition_files, self.node_docs_dir
            )
            
            # ìì‹ ë…¸ë“œë“¤ ì €ì¥
            children_save_success = await self.save_updated_children(updated_children)
            
            # 3ë‹¨ê³„: ë¶€ëª¨ ë…¸ë“œ ìµœì¢… ì—…ë°ì´íŠ¸ (ì—…ë°ì´íŠ¸ëœ ìì‹ ì •ë³´ ë°˜ì˜)
            update_data = self.data_loader.load_for_update(node_data)
            final_extracted = await self.data_processor.process_synthesis_update(
                update_data, updated_children, parent_extracted, node_title
            )
            
            # 4. ë¶€ëª¨ ë…¸ë“œ ì €ì¥ ë° status ì—…ë°ì´íŠ¸
            node_file_path = self.data_loader._get_node_file_path(node_data)
            save_success = self.data_saver.save_to_extraction_section(node_file_path, final_extracted)
            
            if save_success:
                status_success = self.data_saver.update_node_status(node_file_path, True)
                
                node_time = time.time() - node_start
                self.logger.log_operation(f"ë¶€ëª¨íŒŒì´í”„ë¼ì¸ì™„ë£Œ_{node_title}", 
                                        "ì„±ê³µ" if status_success else "ì €ì¥ì„±ê³µ_ìƒíƒœì‹¤íŒ¨", 
                                        {"ì²˜ë¦¬ì‹œê°„": f"{node_time:.2f}ì´ˆ", 
                                         "ìì‹ì €ì¥": "ì„±ê³µ" if children_save_success else "ì‹¤íŒ¨"})
                return status_success
            
            return False
            
        except Exception as e:
            self.logger.log_error(f"ë¶€ëª¨íŒŒì´í”„ë¼ì¸ì˜¤ë¥˜_{node_title}", e)
            return False
    
    async def save_updated_children(self, updated_children: Dict[str, Dict[str, str]]) -> bool:
        """ì—…ë°ì´íŠ¸ëœ ìì‹ ë…¸ë“œë“¤ ì €ì¥"""
        success_count = 0
        
        for file_name, comp_data in updated_children.items():
            if comp_data:
                comp_file_path = self.node_docs_dir / file_name
                if self.data_saver.save_to_extraction_section(comp_file_path, comp_data):
                    success_count += 1
        
        return success_count == len([data for data in updated_children.values() if data])
    
    def verify_nodes_completion(self, nodes: List[Dict[str, Any]]) -> bool:
        """ë…¸ë“œë“¤ì˜ ì™„ë£Œ ìƒíƒœ ê²€ì¦ (process_statusê°€ trueì¸ì§€ í™•ì¸)"""
        for node_data in nodes:
            node_file_path = self.data_loader._get_node_file_path(node_data)
            if not self.data_saver.check_node_status(node_file_path):
                node_title = node_data.get("title", "")
                self.logger.log_error(f"ë…¸ë“œì™„ë£Œê²€ì¦_{node_title}", "statusê°€ trueê°€ ì•„ë‹˜")
                return False
        
        return True
    
    def collect_level_results(self, category: Dict[str, List[Dict[str, Any]]]) -> Dict[str, bool]:
        """ë ˆë²¨ë³„ ê²°ê³¼ ìˆ˜ì§‘"""
        results = {}
        
        for node_data in category["leaf_nodes"] + category["parent_nodes"]:
            node_title = node_data.get("title", "")
            node_file_path = self.data_loader._get_node_file_path(node_data)
            results[node_title] = self.data_saver.check_node_status(node_file_path)
        
        return results


async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    json_path = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-14/nodes.json"
    node_docs_dir = "/home/nadle/projects/Knowledge_Sherpa/v2/25-08-14/node_docs"
    
    print("=" * 60)
    print("ì •ë°˜í•© ë°©ë²•ë¡  ì‹œìŠ¤í…œ V6 - ì˜ì¡´ì„± ê¸°ë°˜ ì²˜ë¦¬ ë° ëª¨ë“ˆí™”ëœ DataLoader")
    print("=" * 60)
    
    # ì •ë°˜í•© í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰ - ë…¸ë“œ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ì „ë‹¬
    processor = DialecticalSynthesisProcessor(node_docs_dir)
    results = await processor.process_nodes_from_json(json_path)
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    success_count = sum(1 for success in results.values() if success)
    print(f"  - ì„±ê³µ: {success_count}/{len(results)}")
    print(f"  - ì‹¤íŒ¨: {len(results) - success_count}/{len(results)}")
    
    if results:
        print(f"\nğŸ“‹ ì²˜ë¦¬ëœ ë…¸ë“œë“¤:")
        for node_title, success in results.items():
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {node_title}")


if __name__ == "__main__":
    asyncio.run(main())