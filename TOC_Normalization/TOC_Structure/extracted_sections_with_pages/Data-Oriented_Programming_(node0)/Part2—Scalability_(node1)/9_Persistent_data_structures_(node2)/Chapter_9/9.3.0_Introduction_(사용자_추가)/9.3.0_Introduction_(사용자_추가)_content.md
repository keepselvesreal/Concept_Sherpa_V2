# 9.3.0 Introduction (사용자 추가)

**페이지**: 208-209
**계층**: Data-Oriented Programming (node0) > Part2—Scalability (node1) > 9 Persistent data structures (node2) > Chapter 9
**추출 시간**: 2025-08-06 19:47:06

---


--- 페이지 208 ---

180 CHAPTER 9 Persistent data structures
Theo I understand how to use structural sharing at the level of the data structure for
linked lists and prepend operations, but how would it work with operations
like appending or modifying an element in a list?
Joe For that purpose, we need to be smarter and represent our list as a tree.
Theo How does that help?
Joe It helps because when a list is represented as a tree, most of the nodes in the
tree can be shared between two versions of the list.
Theo I am totally confused.
Joe Imagine that you take a list with 100,000 elements and split it into two lists of
50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to
99,999 in list 2. How many operations would you need to create a new version
of the list where a single element—let’s say, element at index 75,100—is
modified?
It’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard
and draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to
answer Joe’s question.
List «Next»
List
List 1 List 2
«Next»
0...49,999 50,000...99,999
List 2
Figure 9.2 Structural sharing when
50,000...99,999
a list of 100,000 elements is split
Theo List 1 could be shared with one operation. I’d need to create a new version of
list 2, where element 75,100 is modified. It would take 50,000 operations, so it’s
one operation of sharing and one operation of copying 50,000 elements. Over-
all, it’s 50,001 operations.
Joe Correct. You see that by splitting our original list into two lists, we can create a
new version of the list with a number of operations in the order of the size of
the list divided by 2.
Theo I agree, but 50,000 is still a big number.
Joe Indeed, but nobody prevents us from applying the same trick again, splitting
list 1 and list 2 in two lists each.
Theo How exactly?
Joe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements
25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-
ments 75,000 to 99,999.
Theo Can you draw that on the blackboard?
Joe Sure.

--- 페이지 208 끝 ---


--- 페이지 209 ---

9.2 The efficiency of persistent data structures 181
Now, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.
«Next»
List
List
«Next»
List 1 List 2 List 2
List 1.1 List 1.2 List 2.1 List 2.2 «Next»
0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2
75,000...99,999
Figure 9.3 Structural sharing when a list of 100,000 elements is split twice
Theo Let me count the number of operations for updating a single element. It takes
2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it
takes 25,002 operations to create a new version of the list.
Joe Correct!
Theo Let’s split the list again then!
Joe Absolutely. In fact, we can split the list again and again until the size of the
lists is at most 2. Can you guess what is the complexity of creating a new ver-
sion then?
Theo I’d say around log2 N operations.
Joe I see that you remember well your material from school. Do you have a gut
feeling about what is log2 N when N is 100,000?
Theo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is
128. So, it should be a bit less than 17.
Joe It’s 16.6 to be precise. It means that in order to update an element in a per-
sistent list of 100,000 elements, we need around 17 operations. The same goes
for accessing elements.
Theo Nice, but 17 is still not negligible.
Joe I agree. We can easily improve the performance of accessing elements by using
a higher branching factor in our tree.
Theo What do you mean?
Joe Instead of splitting by 2 at each level, we could split by 32.
Theo But the running time of our algorithm would still grow with log N.
Joe You’re right. From a theoretical perspective, it’s the same. From a practical
perspective, however, it makes a big difference.
Theo Why?
Joe Because log32 N is 5 times lower than log2 N.

--- 페이지 209 끝 ---
