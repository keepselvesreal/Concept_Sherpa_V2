#!/usr/bin/env python3
"""
PDF ì½˜í…ì¸  ì¶”ì¶œê¸° - Leaf nodeìš©
ìƒì„±ëœ í´ë” êµ¬ì¡°ì—ì„œ leaf nodeë“¤ì„ ì°¾ì•„ PDFì—ì„œ í•´ë‹¹ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ ìƒì„±
"""

import pdfplumber
import re
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
import json
from dataclasses import dataclass

@dataclass
class LeafNodeInfo:
    """Leaf node ì •ë³´"""
    title: str
    folder_path: Path
    parent_path: str
    page_range: Optional[Tuple[int, int]] = None
    content: str = ""
    extraction_status: str = "pending"

class PDFContentExtractor:
    def __init__(self, pdf_path: str, base_dir: str = "/home/nadle/projects/Knowledge_Sherpa/v2/Data_Extraction/Data_Oriented_Programming", toc_file: str = "/home/nadle/projects/Knowledge_Sherpa/v2/TOC_Normalization/normalized_toc_with_node_types_v2.md"):
        self.pdf_path = pdf_path
        self.base_dir = Path(base_dir)
        self.toc_file = Path(toc_file)
        self.leaf_nodes = []
        self.content_mappings = self._get_content_mappings()
        
    def _get_content_mappings(self) -> Dict[str, Tuple[int, int]]:
        """PDF í˜ì´ì§€ ë²”ìœ„ ë§¤í•‘ ì •ë³´"""
        return {
            # Part 1 - Flexibility (31-168)
            "complexity": (31, 53),
            "separation": (54, 70),
            "basic_data": (71, 98),
            "state_management": (99, 118),
            "concurrency": (119, 137),
            "unit_tests": (138, 168),
            
            # Part 2 - Scalability (169-274)
            "data_validation": (169, 190),
            "advanced_concurrency": (191, 202),
            "persistent": (203, 224),
            "database": (225, 247),
            "web_services": (248, 274),
            
            # Part 3 - Maintainability (275-380)
            "advanced_validation": (275, 299),
            "polymorphism": (300, 322),
            "advanced_manipulation": (323, 338),
            "debugging": (339, 380),
            
            # Appendices (381-460)
            "appendix_a": (381, 410),
            "appendix_b": (411, 430),
            "appendix_c": (431, 450),
            "appendix_d": (451, 460)
        }
    
    def discover_leaf_nodes(self) -> None:
        """í´ë” êµ¬ì¡°ë¥¼ ìŠ¤ìº”í•˜ì—¬ leaf nodeë“¤ ë°œê²¬"""
        print("Leaf node í´ë” ìŠ¤ìº” ì¤‘...")
        
        if not self.base_dir.exists():
            print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.base_dir}")
            return
            
        leaf_count = 0
        for root, dirs, files in os.walk(self.base_dir):
            current_path = Path(root)
            
            # metadata íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            metadata_file = current_path / ".folder_metadata.json"
            if metadata_file.exists():
                # metadata íŒŒì¼ì„ ì½ì–´ì„œ leaf nodeì¸ì§€ í™•ì¸
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # is_leafê°€ Trueì¸ í´ë”ë¥¼ ì°¾ìŒ
                    if metadata.get('is_leaf', False):
                        relative_path = current_path.relative_to(self.base_dir)
                        parent_path = str(relative_path.parent) if relative_path.parent != Path('.') else ""
                        
                        leaf_node = LeafNodeInfo(
                            title=metadata.get('title', current_path.name.replace('_', ' ')),
                            folder_path=current_path,
                            parent_path=parent_path
                        )
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì¶”ì •
                        leaf_node.page_range = self._estimate_page_range(leaf_node)
                        
                        self.leaf_nodes.append(leaf_node)
                        leaf_count += 1
                        
                        if leaf_count <= 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸ ì¶œë ¥
                            print(f"  âœ“ {leaf_node.title} -> {leaf_node.page_range}")
                            
                except Exception as e:
                    print(f"  âš ï¸ metadata íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {metadata_file}: {e}")
        
        print(f"âœ… ì´ {leaf_count}ê°œ leaf node ë°œê²¬")
    
    def _estimate_page_range(self, leaf_node: LeafNodeInfo) -> Optional[Tuple[int, int]]:
        """Leaf nodeì˜ í˜ì´ì§€ ë²”ìœ„ ì¶”ì •"""
        title_lower = leaf_node.title.lower()
        parent_lower = leaf_node.parent_path.lower()
        
        # 1. ì±•í„°/ì„¹ì…˜ í‚¤ì›Œë“œë¡œ ë§¤í•‘
        for key, (start, end) in self.content_mappings.items():
            if any(keyword in title_lower or keyword in parent_lower 
                   for keyword in key.split('_')):
                return self._refine_page_range(leaf_node, start, end)
        
        # 2. ë¶€ëª¨ ê²½ë¡œ ê¸°ë°˜ ì¶”ì •
        if "part_1" in parent_lower:
            return self._estimate_by_part(leaf_node, 31, 168)
        elif "part_2" in parent_lower:
            return self._estimate_by_part(leaf_node, 169, 274)
        elif "part_3" in parent_lower:
            return self._estimate_by_part(leaf_node, 275, 380)
        elif "appendices" in parent_lower:
            return self._estimate_by_appendix(leaf_node, 381, 460)
        
        # 3. ê¸°ë³¸ ì¶”ì •ê°’
        return (50, 55)  # ê¸°ë³¸ 5í˜ì´ì§€
    
    def _refine_page_range(self, leaf_node: LeafNodeInfo, chapter_start: int, chapter_end: int) -> Tuple[int, int]:
        """ì±•í„° ë‚´ì—ì„œ ì„¸ë¶€ ì„¹ì…˜ì˜ í˜ì´ì§€ ë²”ìœ„ ì¶”ì •"""
        title_lower = leaf_node.title.lower()
        chapter_pages = chapter_end - chapter_start + 1
        
        # Introduction ì„¹ì…˜
        if "introduction" in title_lower or "ì‚¬ìš©ì_ì¶”ê°€" in leaf_node.title:
            return (chapter_start, chapter_start + 2)
        
        # Summary ì„¹ì…˜
        elif "summary" in title_lower:
            return (chapter_end - 1, chapter_end)
        
        # ë²ˆí˜¸ê°€ ìˆëŠ” ì„¹ì…˜ë“¤ (1.1, 1.2, 2.1 ë“±)
        elif re.search(r'\d+\.\d+', title_lower):
            section_match = re.search(r'(\d+)\.(\d+)', title_lower)
            if section_match:
                section_num = int(section_match.group(2))
                # ì„¹ì…˜ ë²ˆí˜¸ì— ë”°ë¼ í˜ì´ì§€ ë²”ìœ„ ì¶”ì •
                section_pages = max(3, chapter_pages // 6)  # ì±•í„°ë¥¼ ëŒ€ëµ 6ê°œ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ”
                section_start = chapter_start + (section_num - 1) * section_pages + 2
                section_end = min(chapter_end - 1, section_start + section_pages - 1)
                return (section_start, section_end)
        
        # ì¼ë°˜ ì„¹ì…˜
        else:
            section_pages = max(3, chapter_pages // 4)
            return (chapter_start + 2, chapter_start + 2 + section_pages - 1)
    
    def _estimate_by_part(self, leaf_node: LeafNodeInfo, part_start: int, part_end: int) -> Tuple[int, int]:
        """Part ë²”ìœ„ ë‚´ì—ì„œ ì¶”ì •"""
        # ê¸°ë³¸ì ìœ¼ë¡œ 3-5í˜ì´ì§€ í• ë‹¹
        pages = 4
        start_page = part_start + hash(leaf_node.title) % (part_end - part_start - pages)
        return (start_page, start_page + pages - 1)
    
    def _estimate_by_appendix(self, leaf_node: LeafNodeInfo, app_start: int, app_end: int) -> Tuple[int, int]:
        """Appendix ë²”ìœ„ ë‚´ì—ì„œ ì¶”ì •"""
        title_lower = leaf_node.title.lower()
        
        if "appendix_a" in leaf_node.parent_path.lower():
            return self._refine_page_range(leaf_node, 381, 410)
        elif "appendix_b" in leaf_node.parent_path.lower():
            return self._refine_page_range(leaf_node, 411, 430)
        elif "appendix_c" in leaf_node.parent_path.lower():
            return self._refine_page_range(leaf_node, 431, 450)
        elif "appendix_d" in leaf_node.parent_path.lower():
            return (451, 460)
        
        return (app_start, app_start + 3)
    
    def extract_all_content(self) -> None:
        """ëª¨ë“  leaf nodeì˜ ì½˜í…ì¸  ì¶”ì¶œ"""
        if not self.leaf_nodes:
            print("âŒ Leaf nodeê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € discover_leaf_nodes()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        print(f"PDF ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘: {len(self.leaf_nodes)}ê°œ leaf node")
        
        with pdfplumber.open(self.pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"PDF ì´ í˜ì´ì§€: {total_pages}")
            
            success_count = 0
            failed_count = 0
            
            for i, leaf_node in enumerate(self.leaf_nodes, 1):
                print(f"[{i}/{len(self.leaf_nodes)}] {leaf_node.title}", end=" ")
                
                if not leaf_node.page_range:
                    print("âŒ í˜ì´ì§€ ë²”ìœ„ ì—†ìŒ")
                    leaf_node.extraction_status = "failed"
                    failed_count += 1
                    continue
                
                try:
                    content = self._extract_content(pdf, leaf_node.page_range, total_pages)
                    if content.strip():
                        leaf_node.content = content
                        leaf_node.extraction_status = "success"
                        self._save_content_file(leaf_node)
                        success_count += 1
                        print(f"âœ… ({leaf_node.page_range[0]}-{leaf_node.page_range[1]})")
                    else:
                        leaf_node.extraction_status = "empty"
                        failed_count += 1
                        print("âŒ ë¹ˆ ì½˜í…ì¸ ")
                        
                except Exception as e:
                    leaf_node.extraction_status = "error"
                    failed_count += 1
                    print(f"âŒ ì˜¤ë¥˜: {str(e)[:50]}")
            
            print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼: âœ… {success_count}ê°œ ì„±ê³µ, âŒ {failed_count}ê°œ ì‹¤íŒ¨")
            self._create_extraction_report(success_count, failed_count)
    
    def _extract_content(self, pdf, page_range: Tuple[int, int], total_pages: int) -> str:
        """ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        start_page, end_page = page_range
        start_page = max(1, start_page) - 1  # 0-based ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        end_page = min(total_pages, end_page) - 1
        
        content = ""
        for page_num in range(start_page, end_page + 1):
            if page_num >= total_pages:
                break
                
            page = pdf.pages[page_num]
            page_text = page.extract_text() or ""
            
            if page_text.strip():
                content += f"=== PAGE {page_num + 1} ===\n{page_text}\n\n"
        
        return content.strip()
    
    def _save_content_file(self, leaf_node: LeafNodeInfo) -> None:
        """Leaf node ì½˜í…ì¸ ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        leaf_node.folder_path.mkdir(parents=True, exist_ok=True)
        
        # content.md íŒŒì¼ ìƒì„±
        content_file = leaf_node.folder_path / "content.md"
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(f"# {leaf_node.title}\n\n")
            f.write(f"**ê²½ë¡œ:** {leaf_node.parent_path}\n")
            f.write(f"**í˜ì´ì§€ ë²”ìœ„:** {leaf_node.page_range[0]}-{leaf_node.page_range[1]}\n")
            f.write(f"**ì¶”ì¶œ ìƒíƒœ:** {leaf_node.extraction_status}\n")
            f.write(f"**ì½˜í…ì¸  ê¸¸ì´:** {len(leaf_node.content)} ë¬¸ì\n\n")
            f.write("---\n\n")
            f.write(leaf_node.content)
        
        # metadata.json íŒŒì¼ ìƒì„±
        metadata = {
            "title": leaf_node.title,
            "parent_path": leaf_node.parent_path,
            "page_range": f"{leaf_node.page_range[0]}-{leaf_node.page_range[1]}" if leaf_node.page_range else "Unknown",
            "extraction_status": leaf_node.extraction_status,
            "content_length": len(leaf_node.content),
            "content_file": "content.md"
        }
        
        metadata_file = leaf_node.folder_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def _create_extraction_report(self, success: int, failed: int) -> None:
        """ì¶”ì¶œ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        report_data = {
            "extraction_summary": {
                "total_leaf_nodes": len(self.leaf_nodes),
                "successful_extractions": success,
                "failed_extractions": failed,
                "success_rate": f"{success}/{len(self.leaf_nodes)} ({100*success//len(self.leaf_nodes) if self.leaf_nodes else 0}%)"
            },
            "leaf_nodes": []
        }
        
        # ê° leaf node ì •ë³´ ì¶”ê°€
        for node in self.leaf_nodes:
            report_data["leaf_nodes"].append({
                "title": node.title,
                "parent_path": node.parent_path,
                "page_range": f"{node.page_range[0]}-{node.page_range[1]}" if node.page_range else "Unknown",
                "status": node.extraction_status,
                "content_length": len(node.content)
            })
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        report_file = self.base_dir.parent / "extraction_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
        text_report_file = self.base_dir.parent / "extraction_report.md"
        with open(text_report_file, 'w', encoding='utf-8') as f:
            f.write("# PDF ì½˜í…ì¸  ì¶”ì¶œ ë³´ê³ ì„œ\n\n")
            f.write(f"**ì¶”ì¶œ ì¼ì‹œ:** 2025-08-06\n")
            f.write(f"**ì´ Leaf Node ìˆ˜:** {len(self.leaf_nodes)}\n")
            f.write(f"**ì„±ê³µì  ì¶”ì¶œ:** {success}ê°œ\n")
            f.write(f"**ì‹¤íŒ¨í•œ ì¶”ì¶œ:** {failed}ê°œ\n")
            f.write(f"**ì„±ê³µë¥ :** {100*success//len(self.leaf_nodes) if self.leaf_nodes else 0}%\n\n")
            
            f.write("## ìƒì„¸ ê²°ê³¼\n\n")
            for node in self.leaf_nodes:
                status_emoji = "âœ…" if node.extraction_status == "success" else "âŒ"
                f.write(f"- {status_emoji} **{node.title}**\n")
                f.write(f"  - ê²½ë¡œ: `{node.parent_path}`\n")
                f.write(f"  - í˜ì´ì§€: {node.page_range[0]}-{node.page_range[1]}\n" if node.page_range else "  - í˜ì´ì§€: Unknown\n")
                f.write(f"  - ìƒíƒœ: {node.extraction_status}\n")
                f.write(f"  - ì½˜í…ì¸  ê¸¸ì´: {len(node.content)} ë¬¸ì\n\n")
        
        print(f"ğŸ“„ ì¶”ì¶œ ë³´ê³ ì„œ ìƒì„±: {text_report_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pdf_path = "/home/nadle/projects/Knowledge_Sherpa/v2/2022_Data-Oriented Programming_Manning.pdf"
    base_dir = "/home/nadle/projects/Knowledge_Sherpa/v2/Data_Extraction/Data_Oriented_Programming"
    
    if not Path(pdf_path).exists():
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return
        
    if not Path(base_dir).exists():
        print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}")
        print("ë¨¼ì € toc_v2_folder_creator.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print("ğŸš€ PDF ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘")
    print(f"ğŸ“– PDF: {pdf_path}")
    print(f"ğŸ“ ê¸°ë³¸ ë””ë ‰í† ë¦¬: {base_dir}")
    
    extractor = PDFContentExtractor(pdf_path, base_dir)
    
    try:
        # Step 1: Leaf node ë°œê²¬
        extractor.discover_leaf_nodes()
        
        # Step 2: ì½˜í…ì¸  ì¶”ì¶œ
        extractor.extract_all_content()
        
        print("\nâœ… PDF ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()