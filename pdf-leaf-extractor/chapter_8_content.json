{
  "chapter": "8",
  "title": "Advanced concurrency control",
  "page_info": {
    "page": 202,
    "title": "Advanced concurrency control",
    "pattern_matched": "Chapter 8",
    "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
  },
  "leaf_nodes": [
    {
      "text": "- 8.0 Introduction (사용자 추가)",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- 8.0 Introduction (사용자 추가) (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 64,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    },
    {
      "text": "- 8.1 The complexity of locks",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- 8.1 The complexity of locks (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 65,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    },
    {
      "text": "- 8.2 Thread-safe counter with atoms",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- 8.2 Thread-safe counter with atoms (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 66,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    },
    {
      "text": "- 8.3 Thread-safe cache with atoms",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- 8.3 Thread-safe cache with atoms (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 67,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    },
    {
      "text": "- 8.4 State management with atoms",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- 8.4 State management with atoms (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 68,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    },
    {
      "text": "- Summary",
      "node_level": 3,
      "part": "Part2—Scalability",
      "chapter": "8 Advanced concurrency control",
      "raw_line": "- Summary (node3) **[LEAF]**",
      "is_part_intro": false,
      "id": 69,
      "chapter_info": {
        "page": 202,
        "title": "Advanced concurrency control",
        "pattern_matched": "Chapter 8",
        "text_preview": "174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do yo"
      },
      "chapter_sections": {
        "start_page": 202,
        "end_page": 232,
        "content": "\n--- Page 202 ---\n174 CHAPTER 8 Advanced concurrency control\nJoe Perfect.\nTheo This atom stuff makes me think about what happened to us yesterday, when we\ntried to call each other at the exact same time.\nJoe What do you mean?\nTheo I don’t know, but I am under the impression that mutexes are like phone calls,\nand atoms are like text messages.\nJoe smiles at Theo but doesn’t reveal the meaning of his smile. After the phone deadlock\nyesterday, Theo’s pretty sure that he and Joe are on the same page.\nSummary\n Managing concurrency with atoms is much simpler than managing concur-\nrency with locks because we don’t have to deal with the risk of deadlocks.\n Cloning data to avoid read locks doesn’t scale.\n When data is immutable, reads are always safe.\n Atoms provide a way to manage concurrency without locks.\n With atoms, deadlocks never happen.\n Using atoms for a thread-safe counter is trivial because the state of the counter\nis represented with a primitive type (an integer).\n We can manage composite data in a thread-safe way with atoms.\n We make the highly scalable state management approach from part 1 thread-\nsafe by keeping the whole system state inside an atom.\n It’s quite common to represent an in-memory cache as a string map.\n When data is immutable, it is safe (and fast) to compare by reference.\n In theory, atoms could create starvation in a system with thousands of threads\nthat do nothing besides swapping an atom.\n In practice, once an atom is swapped, the threads do some real work (e.g.,\ndatabase access) to provide an opportunity for other threads to swap the atom\nsuccessfully.\n--- Page 203 ---\nPersistent data structures\nStanding on the shoulders of giants\nThis chapter covers\n The internal details of persistent data\nstructures\n The time and memory efficiency of persistent\ndata structures\n Using persistent data structures in an\napplication\nIn part 1, we illustrated how to manage the state of a system without mutating data,\nwhere immutability is maintained by constraining ourselves to manipulate the state\nonly with immutable functions using structural sharing. In this chapter, we present\na safer and more scalable way to preserve data immutability—representing data\nwith so-called persistent data structures. Efficient implementations of persistent\ndata structures exist for most programming languages via third-party libraries.\n9.1 The need for persistent data structures\nIt’s at the university where Theo meets Joe this time. When Theo asks Joe if today’s topic\nis academic in nature, Joe tells him that the use of persistent data structures only\nbecame possible in programming languages following a discovery in 2001 by a computer\n175\n--- Page 204 ---\n176 CHAPTER 9 Persistent data structures\nresearcher named Phil Bagwell.1 In 2007, Rich Hickey, the creator of Clojure, used this dis-\ncovery as the foundation of persistent data structures in Clojure. Unveiling the secrets of\nthese data structures to Theo in a university classroom is a way for Joe to honor the mem-\nory of Phil Bagwell, who unfortunately passed away in 2012. When they get to the univer-\nsity classroom, Joe starts the conversation with a question.\nJoe Are you getting used to DOP’s prohibition against mutating data in place and\ncreating new versions instead?\nTheo I think so, but two things bother me about the idea of structural sharing that\nyou showed me.\nJoe What bothers you, my friend?\nTheo Safety and performance.\nJoe What do you mean by safety?\nTheo I mean that using immutable functions to manipulate data doesn’t prevent it\nfrom being modified accidentally.\nJoe Right! Would you like me to show you the naive way to handle immutability or\nthe real way?\nTheo What are the pros and cons of each way?\nJoe The naive way is easy but not efficient, although the real way is efficient but\nnot easy.\nTheo Let’s start with the naive way then.\nJoe Each programming language provides its own way to protect data from being\nmutated.\nTheo How would I do that in Java, for instance?\nJoe Java provides immutable collections, and there is a way to convert a list or a\nmap to an immutable list or an immutable map.\n NOTE Immutable collections are not the same as persistent data structures.\nJoe opens his laptop and fires it up. He brings up two code examples, one for immutable\nlists and one for immutable maps.\nListing9.1 Converting a mutable list to an immutable list in Java\nvar myList = new ArrayList<Integer>();\nmyList.add(1);\nmyList.add(2);\nmyList.add(3);\nvar myImmutableList = List.of(myList.toArray());\n1 P. Bagwell, “Ideal hash trees” (No. REP_WORK), 2001. [Online]. Available: https://lampwww.epfl.ch/papers/\nidealhashtrees.pdf.\n--- Page 205 ---\n9.1 The need for persistent data structures 177\nListing9.2 Converting a mutable map to an immutable map in Java\nvar myMap = new HashMap<String, Object>();\nmyMap.put(\"name\", \"Isaac\");\nmyMap.put(\"age\", 42);\nvar myImmutableMap = Collections.unmodifiableMap(myMap);\nTheo What happens when you try to modify an immutable collection?\nJoe Java throws an UnsupportedOperationException.\nTheo And in JavaScript?\nJoe JavaScript provides an Object.freeze() function that prevents data from\nbeing mutated. It works both with JavaScript arrays and objects.\nJoe takes a minute to scroll through his laptop. When he finds what he’s looking for, he\nshows Theo the code.\nListing9.3 Making an object immutable in JavaScript\nvar a = [1, 2, 3];\nObject.freeze(a);\nvar b = {foo: 1};\nObject.freeze(b);\nTheo What happens when you try to modify a frozen object?\nJoe It depends. In JavaScript strict mode, a TypeError exception is thrown, and in\nnonstrict mode, it fails silently.\n NOTE JavaScript’s strict mode is a way to opt in to a restricted variant of JavaScript\nthat changes some silent errors to throw errors.\nTheo In case of a nested collection, are the nested collections also frozen?\nJoe No, but in JavaScript, one can write a deepFreeze() function that freezes an\nobject recursively. Here’s another example.\nListing9.4 Freezing an object recursively in JavaScript\nfunction deepFreeze(object) {\n// Retrieve the property names defined on object\nconst propNames = Object.getOwnPropertyNames(object);\n// Freeze properties before freezing self\nfor (const name of propNames) {\nconst value = object[name];\nif (value && typeof value === \"object\") {\ndeepFreeze(value);\n}\n}\n--- Page 206 ---\n178 CHAPTER 9 Persistent data structures\nreturn Object.freeze(object);\n}\nTheo I see that it’s possible to ensure that data is never mutated, which answers my\nconcerns about safety. Now, let me share my concerns about performance.\nTIP It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\nJoe Sure.\nTheo If I understand correctly, the main idea behind structural sharing is that most\ndata is usually shared between two versions.\nJoe Correct.\nTheo This insight allows us to create new versions of our collections using a shallow\ncopy instead of a deep copy, and you claimed that it was efficient.\nJoe Exactly!\nTheo Now, here is my concern. In the case of a collection with many entries, a shal-\nlow copy might be expensive.\nJoe Could you give me an example of a collection with many entries?\nTheo A catalog with 100,000 books, for instance.\nJoe On my machine, making a shallow copy of a collection with 100,000 entries\ndoesn’t take more than 50 milliseconds.\nTheo Sometimes, even 50 milliseconds per update isn’t acceptable.\nJoe I totally agree with you. When one needs data immutability at scale, naive struc-\ntural sharing is not appropriate.\nTheo Also, shallow copying an array of 100,000 elements on each update would\nincrease the program memory by 100 KB.\nJoe Indeed, at scale, we have a problem both with memory and computation.\nTIP At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\nTheo Is there a better solution?\nJoe Yes! For that, you’ll need to learn the real way to handle immutability. It’s\ncalled persistent data structures.\n9.2 The efficiency of persistent data structures\nTheo In what sense are those data structures persistent?\nJoe Persistent data structures are so named because they always preserve their pre-\nvious versions.\nTIP Persistent data structures always preserve the previous version of themselves\nwhen they are modified.\nJoe Persistent data structures address the two main limitations of naive structural\nsharing: safety and performance.\n--- Page 207 ---\n9.2 The efficiency of persistent data structures 179\nTheo Let’s start with safety. How do persistent data structures prevent data from\nbeing mutated accidentally?\nJoe In a language like Java, they implement the mutation methods of the collec-\ntion interfaces by throwing the run-time exception UnsupportedOperation-\nException.\nTheo And, in a language like JavaScript?\nJoe In JavaScript, persistent data structures provide their own methods to access\ndata, and none of those methods mutate data.\nTheo Does that mean that we can’t use the dot notation to access fields?\nJoe Correct. Fields of persistent data structures are accessed via a specific API.\nTheo What about efficiency? How do persistent data structures make it possible to\ncreate a new version of a huge collection in an efficient way?\nJoe Persistent data structures organize data in such a way that we can use structural\nsharing at the level of the data structure.\nTheo Could you explain?\nJoe Certainly. Let’s start with the simplest data structure: a linked list. Imagine that\nyou have a linked list with 100,000 elements.\nTheo OK.\nJoe What would it take to prepend an element to the head of the list?\nTheo You mean to create a new version of the list with an additional element?\nJoe Exactly!\nTheo Well, we could copy the list and then prepend an element to the list, but it\nwould be quite expensive.\nJoe What if I tell you that the original linked list is guaranteed to be immutable?\nTheo In that case, I could create a new list with a new head that points to the head of\nthe original list.\nTheo goes to the classroom blackboard. He picks up a piece of chalk and draws the dia-\ngram shown in figure 9.1.\nNew list Original list\nFigure 9.1 Structural sharing\n0 1 2 3 4 5 with linked lists\nJoe Would the efficiency of this operation depend on the size of the list?\nTheo No, it would be efficient, no matter the size of the list.\nJoe That’s what I mean by structural sharing at the level of the data structure itself.\nIt relies on a simple but powerful insight—when data is immutable, it is safe to\nshare it.\nTIP When data is immutable, it is safe to share it.\n--- Page 208 ---\n180 CHAPTER 9 Persistent data structures\nTheo I understand how to use structural sharing at the level of the data structure for\nlinked lists and prepend operations, but how would it work with operations\nlike appending or modifying an element in a list?\nJoe For that purpose, we need to be smarter and represent our list as a tree.\nTheo How does that help?\nJoe It helps because when a list is represented as a tree, most of the nodes in the\ntree can be shared between two versions of the list.\nTheo I am totally confused.\nJoe Imagine that you take a list with 100,000 elements and split it into two lists of\n50,000 elements each: elements 0 to 49,999 in list 1, and elements 50,000 to\n99,999 in list 2. How many operations would you need to create a new version\nof the list where a single element—let’s say, element at index 75,100—is\nmodified?\nIt’s hard for Theo to visualize this kind of stuff mentally. He goes back to the blackboard\nand draws a diagram (see figure 9.2). Once Theo looks at the diagram, it’s easy for him to\nanswer Joe’s question.\nList «Next»\nList\nList 1 List 2\n«Next»\n0...49,999 50,000...99,999\nList 2\nFigure 9.2 Structural sharing when\n50,000...99,999\na list of 100,000 elements is split\nTheo List 1 could be shared with one operation. I’d need to create a new version of\nlist 2, where element 75,100 is modified. It would take 50,000 operations, so it’s\none operation of sharing and one operation of copying 50,000 elements. Over-\nall, it’s 50,001 operations.\nJoe Correct. You see that by splitting our original list into two lists, we can create a\nnew version of the list with a number of operations in the order of the size of\nthe list divided by 2.\nTheo I agree, but 50,000 is still a big number.\nJoe Indeed, but nobody prevents us from applying the same trick again, splitting\nlist 1 and list 2 in two lists each.\nTheo How exactly?\nJoe We can make list 1.1 with elements 0 to 24,999, then list 1.2 with elements\n25,000 to 49,999, list 2.1 with elements 50,000 to 74,999, and list 2.2 with ele-\nments 75,000 to 99,999.\nTheo Can you draw that on the blackboard?\nJoe Sure.\n--- Page 209 ---\n9.2 The efficiency of persistent data structures 181\nNow, it’s Joe that goes to the blackboard. He draws the diagram in figure 9.3.\n«Next»\nList\nList\n«Next»\nList 1 List 2 List 2\nList 1.1 List 1.2 List 2.1 List 2.2 «Next»\n0...24,499 25,000...49,999 50,000...74,999 75,000...99,999 List 2.2\n75,000...99,999\nFigure 9.3 Structural sharing when a list of 100,000 elements is split twice\nTheo Let me count the number of operations for updating a single element. It takes\n2 operations of sharing and 1 operation of copying 25,000 elements. Overall, it\ntakes 25,002 operations to create a new version of the list.\nJoe Correct!\nTheo Let’s split the list again then!\nJoe Absolutely. In fact, we can split the list again and again until the size of the\nlists is at most 2. Can you guess what is the complexity of creating a new ver-\nsion then?\nTheo I’d say around log2 N operations.\nJoe I see that you remember well your material from school. Do you have a gut\nfeeling about what is log2 N when N is 100,000?\nTheo Let me see...2 to the power of 10 is around 1,000, and 2 to the power of 7 is\n128. So, it should be a bit less than 17.\nJoe It’s 16.6 to be precise. It means that in order to update an element in a per-\nsistent list of 100,000 elements, we need around 17 operations. The same goes\nfor accessing elements.\nTheo Nice, but 17 is still not negligible.\nJoe I agree. We can easily improve the performance of accessing elements by using\na higher branching factor in our tree.\nTheo What do you mean?\nJoe Instead of splitting by 2 at each level, we could split by 32.\nTheo But the running time of our algorithm would still grow with log N.\nJoe You’re right. From a theoretical perspective, it’s the same. From a practical\nperspective, however, it makes a big difference.\nTheo Why?\nJoe Because log32 N is 5 times lower than log2 N.\n--- Page 210 ---\n182 CHAPTER 9 Persistent data structures\nTheo That’s true: 2 to the power of 5 is 32.\nJoe Back to our list of 100,000 elements, can you tell me how many operations are\nrequired to access an element if the branching factor is 32?\nTheo With a branching factor of 2, it was 16.6. If I divide 16.6 by 5, I get 3.3.\nJoe Correct!\nTIP By using a branching factor of 32, we make elements accessed in persistent lists\nmore efficient.\nTheo Does this trick also improve the performance of updating an element in a list?\nJoe Yes, indeed, it does.\nTheo How? We’d have to copy 32 elements at each level instead of 2 elements. It’s a\n16× performance hit that’s not compensated for by the fact that the tree depth\nis reduced by 5×!\nJoe I see that you are quite sharp with numbers. There is another thing to take\ninto consideration in our practical analysis of the performance: modern CPU\narchitecture.\nTheo Interesting. The more you tell me about persistent data structures, the more I\nunderstand why you wanted to have this session at a university: it’s because\nwe’re dealing with all this academic stuff.\nJoe Yep. So, to continue, modern CPUs read and write data from and to the main\nmemory in units of cache lines, often 32 or 64 bytes long.\nTheo What difference does that make?\nJoe A nice consequence of this data access pattern is that copying an array of size\n32 is much faster than copying 16 arrays of size 2 that belong to different levels\nof the tree.\nTheo Why is that?\nJoe The reason is that copying an array of size 32 can be done in a single pair of\ncache accesses: one for read and one for write. Although for arrays that belong\nto different tree levels, each array requires its own pair of cache accesses, even\nif there are only 2 elements in the array.\nTheo In other words, the performance of updating a persistent list is dominated by\nthe depth of the tree.\nTIP In modern CPU architectures, the performance of updating a persistent list is\ndominated much more by the depth of the tree than by the number of nodes at each\nlevel of the tree.\nJoe That’s correct, up to a certain point. With today’s CPUs, using a branching fac-\ntor of 64 would, in fact, decrease the performance of update operations.\nTheo I see.\nJoe Now, I am going to make another interesting claim that is not accurate from a\ntheoretical perspective but accurate in practice.\nTheo What is it?\n--- Page 211 ---\n9.2 The efficiency of persistent data structures 183\nJoe The number of operations it takes to get or update an element in a persistent\nlist with branching factor 32 is constant.\nTheo How can that be? You just made the point that the number of operations is\nlog32 N.\nJoe Be patient, my friend. What is the highest number of elements that you can\nhave in a list, in practice?\nTheo I don’t know. I never thought about that.\nJoe Let’s assume that it takes 4 bytes to store an element in a list.\nTheo OK.\nJoe Now, can you tell me how much memory it would take to hold a list with 10 bil-\nlion elements?\nTheo You mean 1 with 10 zeros?\nJoe Yes.\nTheo Each element take 4 bytes, so it would be around 40 GB!\nJoe Correct. Do you agree that it doesn’t make sense to hold a list that takes 40 GB\nof memory?\nTheo I agree.\nJoe So let’s take 10 billion as an upper bound to the number of elements in a list.\nWhat is log32 of 10 billion?\nOnce again, Theo uses the blackboard to clarify his thoughts. With that, he quickly finds\nthe answer.\nTheo 1 billion is approximately 2^30. Therefore, 10 billion is around 2^33. That\nmeans that log2 of 10 billion is 33, so log32 of 10 billion should be around\n33/5, which is a bit less than 7.\nJoe I am impressed again by your sharpness with numbers. To be precise, log32 of\n10 billion is 6.64.\nTheo (smiling) I didn’t get that far.\nJoe Did I convince you that, in practice, accessing or updating an element in a per-\nsistent list is essentially constant?\nTheo Yes, and I find it quite amazing!\nTIP Persistent lists can be manipulated in near constant time.\nJoe Me too.\nTheo What about persistent maps?\nJoe It’s quite similar, but I don’t think we have time to discuss it now.\nStartled, Theo looks at his watch. This morning’s session has gone by so quickly. He notices\nthat it’s time to get back to the office and have lunch.\n--- Page 212 ---\n184 CHAPTER 9 Persistent data structures\n9.3 Persistent data structures libraries\nOn their way back to the office, Theo and Joe don’t talk too much. Theo’s thoughts take\nhim back to what he learned in the university classroom. He feels a lot of respect for Phil\nBagwell, who discovered how to manipulate persistent data structures efficiently, and for\nRich Hickey, who created a programming language incorporating that discovery as a core\nfeature and making it available to the world. Immediately after lunch, Theo asks Joe to\nshow him what it looks like to manipulate persistent data structures for real in a program-\nming language.\nTheo Are persistent data structures available in all programming languages?\nJoe A few programming languages like Clojure, Scala, and C# provide them as part\nof the language. In most programming languages, though, you need a third-\nparty library.\nTheo Could you give me a few references?\nJoe Sure.\nUsing Theo’s laptop, Joe bookmarks some sites. He knows exactly which URLs to look for.\nThen, while Theo is looking over the bookmarked sites, Joe goes to the whiteboard and\njots down the specific libraries in table 9.1.\n Immutable.js for JavaScript at https://immutable-js.com/\n Paguro for Java at https://github.com/GlenKPeterson/Paguro\n Immutable Collections for C# at http://mng.bz/QW51\n Pyrsistent for Python at https://github.com/tobgu/pyrsistent\n Hamster for Ruby at https://github.com/hamstergem/hamster\nTable 9.1 Persistent data structure libraries\nLanguage Library\nJavaScript Immutable.js\nJava Paguro\nC# Provided by the language\nPython Pyrsistent\nRuby Hamster\nTheo What does it take to integrate persistent data structures provided by a third-\nparty library into your code?\n9.3.1 Persistent data structures in Java\nJoe In an object-oriented language like Java, it’s quite straightforward to integrate\npersistent data structures in a program because persistent data structures\nimplement collection interfaces, besides the parts of the interface that mutate\nin place.\nTheo What do you mean?\n--- Page 213 ---\n9.3 Persistent data structures libraries 185\nJoe Take for instance, Paguro for Java. Paguro persistent maps implement the\nread-only methods of java.util.Map like get() and containsKey(), but not\nmethods like put() and remove(). On the other hand, Paguro vectors imple-\nment the read-only methods of java.util.List like get() and size(), but not\nmethods like set().\nTheo What happens when we call put() or remove() on a Paguro map?\nJoe It throws an UnSupportedOperationException exception.\nTheo What about iterating over the elements of a Paguro collection with a forEach()?\nJoe That works like it would in any Java collection. Here, let me show you an example.\nListing9.5 Iterating over a Paguro vector\nvar myVec = PersistentVector.ofIter(\nList.of(10, 2, 3));\nCreates a Paguro\nvector from a\nfor (Integer i : myVec) {\nJava list\nSystem.out.println(i);\n}\nTheo What about Java streams?\nJoe Paguro collections are Java collections, so they support the Java stream inter-\nface. Take a look at this code.\nListing9.6 Streaming a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvec1.stream().sorted().map(x -> x + 1);\nTIP Paguro collections implement the read-only parts of Java collection interfaces.\nTherefore, they can be passed to any methods that expect to receive a Java collection\nwithout mutating it.\nTheo So far, you told me how do use Paguro collections as Java read-only collections.\nHow do I make modifications to Paguro persistent data structures?\nJoe In a way similar to the _.set() function of Lodash FP that we talked about\nearlier. Instead of mutating in place, you create a new version.\nTheo What methods does Paguro expose for creating new versions of a data structure?\nJoe For vectors, you use replace(), and for maps, you use assoc().\nListing9.7 Creating a modified version of a Paguro vector\nvar myVec = PersistentVector.ofIter(List.of(10, 2, 3));\nvar myNextVec = myVec.replace(0, 42);\n--- Page 214 ---\n186 CHAPTER 9 Persistent data structures\nListing9.8 Creating a modified version of a Paguro map\nvar myMap = PersistentHashMap.of(Map.of(\"aa\", 1, \"bb\", 2)\n.entrySet());\nCreates a Paguro map\nfrom a Java map entry set\nvar myNextMap = myMap.assoc(\"aa\", 42);\nTheo Yes! Now I see how to use persistent data structures in Java, but what about\nJavaScript?\n9.3.2 Persistent data structures in JavaScript\nJoe In a language like JavaScript, it’s a bit more cumbersome to integrate per-\nsistent data structures.\nTheo How so?\nJoe Because JavaScript objects and arrays don’t expose any interface.\nTheo Bummer.\nJoe It’s not as terrible as it sounds because Immutable.js exposes its own set of\nfunctions to manipulate its data structures.\nTheo What do you mean?\nJoe I’ll show you in a moment. But first, let me show you how to initiate Immutable.js\npersistent data structures.\nTheo OK!\nJoe Immutable.js provides a handy function that recursively converts a native data\nobject to an immutable one. It’s called Immutable.fromJS().\nTheo What do you mean by recursively?\nJoe Consider the map that holds library data from our Library Management Sys-\ntem: it has values that are themselves maps. Immutable.fromJS() converts the\nnested maps into immutable maps.\nTheo Could you show me some code?\nJoe Absolutely. Take a look at this JavaScript code for library data.\nListing9.9 Conversion to immutable data\nvar libraryData = Immutable.fromJS({\n\"catalog\": {\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n--- Page 215 ---\n9.3 Persistent data structures libraries 187\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n}\n});\nTheo Do you mean that the catalog value in libraryData map is itself an immutable\nmap?\nJoe Yes, and the same for booksByIsbn, authorIds, and so forth.\nTheo Cool! So how do I access a field inside an immutable map?\nJoe As I told you, Immutable.js provides its own API for data access. For instance,\nin order to access a field inside an immutable map, you use Immutable.get()\nor Immutable.getIn() like the following.\nListing9.10 Accessing a field and a nested field in an immutable map\nImmutable.get(libraryData, \"catalog\");\nImmutable.getIn(libraryData,\n[\"catalog\", \"booksByIsbn\", \"978-1779501127\", \"title\"]);\n// → \"Watchmen\"\nTheo How do I make a modification to a map?\nJoe Similar to what we did with Lodash FP, you use an Immutable.set() or\nImmutable.setIn() map to create a new version of the map where a field is\nmodified. Here’s how.\nListing9.11 Creating a new version of a map where a field is modified\nImmutable.setIn(libraryData,\n[\"catalog\", \"booksByIsbn\",\n\"978-1779501127\", \"publicationYear\"],\n1988);\nTheo What happens when I try to access a field in the map using JavaScript’s dot or\nbracket notation?\nJoe You access the internal representation of the map instead of accessing a map\nfield.\nTheo Does that mean that we can’t pass data from Immutable.js to Lodash for data\nmanipulation?\nJoe Yes, but it’s quite easy to convert any immutable collection into a native Java-\nScript object back and forth.\nTheo How?\nJoe Immutable.js provides a toJS() method to convert an arbitrary deeply nested\nimmutable collection into a JavaScript object.\n--- Page 216 ---\n188 CHAPTER 9 Persistent data structures\nTheo But if I have a huge collection, it could take lots of time to convert it, right?\nJoe True. We need a better solution. Hopefully, Immutable.js provides its own set\nof data manipulation functions like map(), filter(), and reduce().\nTheo What if I need more data manipulation like Lodash’s _.groupBy()?\nJoe You could write your own data manipulation functions that work with the\nImmutable.js collections or use a library like mudash, which provides a port of\nLodash to Immutable.js.\n NOTE You can access the mudash library at https://github.com/brianneisler/mudash.\nTheo What would you advise?\nJoe A cup of coffee, then I’ll show you how to port functions from Lodash to\nImmutable.js and how to adapt the code from your Library Management System.\nYou can decide on whichever approach works best for your current project.\n9.4 Persistent data structures in action\nJoe Let’s start with our search query. Can you look at the current code and tell me\nthe Lodash functions that we used to implement the search query?\nTheo Including the code for the unit tests?\nJoe Of course!\n NOTE See chapter 6 for the unit test of the search query.\n9.4.1 Writing queries with persistent data structures\nTheo The Lodash functions we used were get, map, filter, and isEqual.\nJoe Here’s the port of those four functions from Lodash to Immutable.js.\nListing9.12 Porting some functions from Lodash to Immutable.js\nImmutable.map = function(coll, f) {\nreturn coll.map(f);\n};\nImmutable.filter = function(coll, f) {\nif(Immutable.isMap(coll)) {\nreturn coll.valueSeq().filter(f);\n}\nreturn coll.filter(f);\n};\nImmutable.isEqual = Immutable.is;\nTheo The code seems quite simple. But can you explain it to me, function by function?\nJoe Sure. Let’s start with get. For accessing a field in a map, Immutable.js provides\ntwo functions: get for direct fields and getIn for nested fields. It’s different\nfrom Lodash, where _.get works both on direct and nested fields.\n--- Page 217 ---\n9.4 Persistent data structures in action 189\nTheo What about map?\nJoe Immutable.js provides its own map function. The only difference is that it is a\nmethod of the collection, but it is something that we can easily adapt.\nTheo What about filter? How would you make it work both for arrays and maps\nlike Lodash’s filter?\nJoe Immutable.js provides a valueSeq method that returns the values of a map.\nTheo Cool. And what about isEqual to compare two collections?\nJoe That’s easy. Immutable.js provides a function named is that works exactly as\nisEqual.\nTheo So far, so good. What do I need to do now to make the code of the search\nquery work with Immutable.js?\nJoe You simply replace each occurrence of an _ with Immutable; _.map becomes\nImmutable.map, _.filter becomes Immutable.filter, and _.isEqual\nbecomes Immutable.isEqual.\nTheo I can’t believe it’s so easy!\nJoe Try it yourself; you’ll see. Sometimes, it’s a bit more cumbersome because\nyou need to convert the JavaScript objects to Immutable.js objects using\nImmutable.fromJS.\nTheo copies and pastes the snippets for the code and the unit tests of the search query.\nThen, he uses his IDE to replace the _ with Immutable. When Theo executes the tests and\nthey pass, he is surprised but satisfied. Joe smiles.\nListing9.13 Implementing book search with persistent data structures\nclass Catalog {\nstatic authorNames(catalogData, authorIds) {\nreturn Immutable.map(authorIds, function(authorId) {\nreturn Immutable.getIn(\ncatalogData,\n[\"authorsById\", authorId, \"name\"]);\n});\n}\nstatic bookInfo(catalogData, book) {\nvar bookInfo = Immutable.Map({\n\"title\": Immutable.get(book, \"title\"),\n\"isbn\": Immutable.get(book, \"isbn\"),\n\"authorNames\": Catalog.authorNames(\ncatalogData,\nImmutable.get(book, \"authorIds\"))\n});\nreturn bookInfo;\n}\nstatic searchBooksByTitle(catalogData, query) {\nvar allBooks = Immutable.get(catalogData, \"booksByIsbn\");\nvar queryLowerCased = query.toLowerCase();\nvar matchingBooks = Immutable.filter(allBooks, function(book) {\n--- Page 218 ---\n190 CHAPTER 9 Persistent data structures\nreturn Immutable.get(book, \"title\").\ntoLowerCase().\nincludes(queryLowerCased);\n});\nvar bookInfos = Immutable.map(matchingBooks, function(book) {\nreturn Catalog.bookInfo(catalogData, book);\n});\nreturn bookInfos;\n}\n}\nListing9.14 Testing book search with persistent data structures\nvar catalogData = Immutable.fromJS({\n\"booksByIsbn\": {\n\"978-1779501127\": {\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"publicationYear\": 1987,\n\"authorIds\": [\"alan-moore\",\n\"dave-gibbons\"]\n}\n},\n\"authorsById\": {\n\"alan-moore\": {\n\"name\": \"Alan Moore\",\n\"bookIsbns\": [\"978-1779501127\"]\n},\n\"dave-gibbons\": {\n\"name\": \"Dave Gibbons\",\n\"bookIsbns\": [\"978-1779501127\"]\n}\n}\n});\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Watchmen\"),\nImmutable.fromJS([bookInfo]));\n// → true\nImmutable.isEqual(\nCatalog.searchBooksByTitle(catalogData, \"Batman\"),\nImmutable.fromJS([]));\n// → true\n--- Page 219 ---\n9.4 Persistent data structures in action 191\n9.4.2 Writing mutations with persistent data structures\nTheo Shall we move forward and port the add member mutation?\nJoe Sure. Porting the add member mutation from Lodash to Immutable.js only\nrequires you to again replace the underscore (_) with Immutable. Let’s look at\nsome code.\nListing9.15 Implementing member addition with persistent data structures\nUserManagement.addMember = function(userManagement, member) {\nvar email = Immutable.get(member, \"email\");\nvar infoPath = [\"membersByEmail\", email];\nif(Immutable.hasIn(userManagement, infoPath)) {\nthrow \"Member already exists.\";\n}\nvar nextUserManagement = Immutable.setIn(userManagement,\ninfoPath,\nmember);\nreturn nextUserManagement;\n};\nTheo So, for the tests, I’d convert the JavaScript objects to Immutable.js objects with\nImmutable.fromJS(). How does this look?\nListing9.16 Testing member addition with persistent data structures\nvar jessie = Immutable.fromJS({\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n});\nvar franck = Immutable.fromJS({\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n});\nvar userManagementStateBefore = Immutable.fromJS({\n\"membersByEmail\": {\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n}\n}\n});\nvar expectedUserManagementStateAfter = Immutable.fromJS({\n\"membersByEmail\": {\n\"jessie@gmail.com\": {\n\"email\": \"jessie@gmail.com\",\n\"password\": \"my-secret\"\n},\n\"franck@gmail.com\": {\n\"email\": \"franck@gmail.com\",\n\"password\": \"my-top-secret\"\n--- Page 220 ---\n192 CHAPTER 9 Persistent data structures\n}\n}\n});\nvar result = UserManagement.addMember(userManagementStateBefore, jessie);\nImmutable.isEqual(result, expectedUserManagementStateAfter);\n// → true\nJoe Great!\n9.4.3 Serialization and deserialization\nTheo Does Immutable.js also support JSON serialization and deserialization?\nJoe It supports serialization out of the box. As for deserialization, we need to write\nour own function.\nTheo Does Immutable.js provide an Immutable.stringify() function?\nJoe That’s not necessary because the native JSON.stringify() function works\nwith Immutable.js objects. Here’s another example.\nListing9.17 JSON serialization of an Immutable.js collection\nvar bookInfo = Immutable.fromJS({\n\"isbn\": \"978-1779501127\",\n\"title\": \"Watchmen\",\n\"authorNames\": [\"Alan Moore\",\n\"Dave Gibbons\"]\n});\nJSON.stringify(bookInfo);\n// → {\\\"isbn\\\":\\\"978-1779501127\\\",\\\"title\\\":\\\"Watchmen\\\",\n// → \\\"authorNames\\\":[\\\"Alan Moore\\\",\\\"Dave Gibbons\\\"]}\nTheo How does JSON.stringify() know how to handle an Immutable.js collection?\nJoe As an OOP developer, you shouldn’t be surprised by that.\nTheo Hmm...let me think a minute. OK, here’s my guess. Is that because JSON\n.stringify() calls some method on its argument?\nJoe Exactly! If the object passed to JSON.stringify() has a .toJSON() method,\nit’s called by JSON.stringify().\nTheo Nice. What about JSON deserialization?\nJoe That needs to be done in two steps. You first convert the JSON string to a Java-\nScript object and then to an immutable collection.\nTheo Something like this piece of code?\nListing9.18 Converting a JSON string into an immutable collection\nImmutable.parseJSON = function(jsonString) {\nreturn Immutable.fromJS(JSON.parse(jsonString));\n};\nJoe Exactly.\n--- Page 221 ---\n9.4 Persistent data structures in action 193\n9.4.4 Structural diff\nTheo So far, we have ported pieces of code that dealt with simple data manipula-\ntions. I’m curious to see how it goes with complex data manipulations such as\nthe code that computes the structural diff between two maps.\n NOTE Chapter 5 introduces structural diff.\nJoe That also works smoothly, but we need to port another eight functions.\nListing9.19 Porting Lodash functions involved in structural diff computation\nImmutable.reduce = function(coll, reducer, initialReduction) {\nreturn coll.reduce(reducer, initialReduction);\n};\nImmutable.isEmpty = function(coll) {\nreturn coll.isEmpty();\n};\nImmutable.keys = function(coll) {\nreturn coll.keySeq();\n};\nImmutable.isObject = function(coll) {\nreturn Immutable.Map.isMap(coll);\n};\nImmutable.isArray = Immutable.isIndexed;\nImmutable.union = function() {\nreturn Immutable.Set.union(arguments);\n};\nTheo Everything looks trivial with one exception: the use of arguments in Immutable\n.union.\nJoe In JavaScript, arguments is an implicit array-like object that contains the values\nof the function arguments.\nTheo I see. It’s one of those pieces of JavaScript magic!\nJoe Yep. We need to use arguments because Lodash and Immutable.js differ slightly\nin the signature of the union function. Immutable.Set.union receives an array\nof lists, whereas a Lodash _.union receives several arrays.\nTheo Makes sense. Let me give it a try.\nBlowing on his fingers like a seasoned safecracker, first one hand and then the next, Theo\nbegins typing. Once again, Theo is surprised to discover that after replacing the _ with\nImmutable in listing 9.20, the tests pass with the code in listing 9.21.\nListing9.20 Implementing structural diff with persistent data structures\nfunction diffObjects(data1, data2) {\nvar emptyObject = Immutable.isArray(data1) ?\nImmutable.fromJS([]) :\n--- Page 222 ---\n194 CHAPTER 9 Persistent data structures\nImmutable.fromJS({});\nif(data1 == data2) {\nreturn emptyObject;\n}\nvar keys = Immutable.union(Immutable.keys(data1), Immutable.keys(data2));\nreturn Immutable.reduce(keys,\nfunction (acc, k) {\nvar res = diff(Immutable.get(data1, k),\nImmutable.get(data2, k));\nif((Immutable.isObject(res) && Immutable.isEmpty(res)) ||\n(res == \"data-diff:no-diff\")) {\nreturn acc;\n}\nreturn Immutable.set(acc, k, res);\n},\nemptyObject);\n}\nfunction diff(data1, data2) {\nif(Immutable.isObject(data1) && Immutable.isObject(data2)) {\nreturn diffObjects(data1, data2);\n}\nif(data1 !== data2) {\nreturn data2;\n}\nreturn \"data-diff:no-diff\";\n}\nListing9.21 Testing structural diff with persistent data structures\nvar data1 = Immutable.fromJS({\ng: {\nc: 3\n},\nx: 2,\ny: {\nz: 1\n},\nw: [5]\n});\nvar data2 = Immutable.fromJS({\ng: {\nc:3\n},\nx: 2,\ny: {\nz: 2\n},\nw: [4]\n});\nImmutable.isEqual(diff(data1, data2),\nImmutable.fromJS({\n--- Page 223 ---\nSummary 195\n\"w\": [\n4\n],\n\"y\": {\n\"z\": 2\n}\n}));\nJoe What do you think of all this, my friend?\nTheo I think that using persistent data collections with a library like Immutable.js is\nmuch easier than understanding the internals of persistent data structures. But\nI’m also glad that I know how it works under the hood.\nAfter accompanying Joe to the office door, Theo meets Dave. Dave had been peering\nthrough the window in Theo’s office, looking at the whiteboard, anxious to catch a glimpse\nof today’s topic on DOP.\nDave What did Joe teach you today?\nTheo He took me to the university and taught me the foundations of persistent data\nstructures for dealing with immutability at scale.\nDave What’s wrong with the structural sharing that I implemented a couple of\nmonths ago?\nTheo When the number of elements in the collection is big enough, naive structural\nsharing has performance issues.\nDave I see. Could you tell me more about that?\nTheo I’d love to, but my brain isn’t functioning properly after this interesting but\nexhausting day. We’ll do it soon, promise.\nDave No worries. Have a nice evening, Theo.\nTheo You too, Dave.\nSummary\n It’s possible to manually ensure that our data isn’t mutated, but it’s cumbersome.\n At scale, naive structural sharing causes a performance hit, both in terms of\nmemory and computation.\n Naive structural sharing doesn’t prevent data structures from being accidentally\nmutated.\n Immutable collections are not the same as persistent data structures.\n Immutable collections don’t provide an efficient way to create new versions of\nthe collections.\n Persistent data structures protect data from mutation.\n Persistent data structures provide an efficient way to create new versions of the\ncollections.\n Persistent data structures always preserve the previous version of themselves when\nthey are modified.\n--- Page 224 ---\n196 CHAPTER 9 Persistent data structures\n Persistent data structures represent data internally in such a way that structural\nsharing scales well, both in terms of memory and computation.\n When data is immutable, it is safe to share it.\n Internally, persistence uses a branching factor of 32.\n In practice, manipulation of persistent data structures is efficient even for col-\nlections with 10 billion entries!\n Due to modern architecture considerations, the performance of updating a\npersistent list is dominated much more by the depth of the tree than by the\nnumber of nodes at each level of the tree.\n Persistent lists can be manipulated in near constant time.\n In most languages, third-party libraries provide an implementation of persistent\ndata structures.\n Paguro collections implement the read-only parts of Java collection interfaces.\n Paguro collections can be passed to any methods that expect to receive a Java\ncollection without mutating them.\n--- Page 225 ---\nDatabase operations\nA cloud is a cloud\nThis chapter covers\n Fetching data from the database\n Storing data in the database\n Manipulating data fetched from the database\nTraditionally in OOP, we use design patterns and complex layers of objects to struc-\nture access to the database. In DOP, we prefer to represent data fetched from the\ndatabase with generic data collections, namely, lists of maps, where fields in the\nmaps correspond to database column values. As we’ll see throughout the chapter,\nthe fact that fields inside a map are accessible dynamically via their names allows us\nto use the same generic code for different data entities.\nTIP The best way to manipulate data is to represent data as data.\nIn this chapter, we’ll illustrate the application of data-oriented principles when\naccessing data from a relational database. Basic knowledge of relational database\nand SQL query syntax (like SELECT, AS, WHERE, and INNER JOIN) is assumed. This\napproach can be easily adapted to NoSQL databases.\n197\n--- Page 226 ---\n198 CHAPTER 10 Database operations\nApplications that run on the server usually store data in a database. In DOP, we\nrepresent data retrieved from the database the same way we represent any other data\nin our application—with generic data collections. This leads to\n Reduced system complexity.\n Increased genericity.\n10.1 Fetching data from the database\nTheo and Joe go for a walk in a park near the office. They sit on a bench close to a beau-\ntiful lake and gaze at the clouds in the sky. After a couple of minutes of meditative\nsilence, Joe asks Theo, “What do you see?” Theo tells him that this cloud looks to him\nlike a horse, and that one looks like a car. On their way back to the office, Theo asks Joe\nfor an explanation about the clouds. Joe answers with a mysterious smile on his lips, “A\ncloud is a cloud.”\nTheo So far you’ve shown me how DOP represents data that lives in the memory of\nthe application. What about data that comes from the outside?\nJoe What do you mean by outside?\nTheo Data that comes from the database.\nJoe I’ll return the question to you. How do you think that we should represent data\nthat comes from the database in DOP?\nTheo As generic data collections, I guess.\nJoe Exactly! In DOP, we always represent data with generic data collections.\nTheo Does that mean that we can manipulate data from the database with the same\nflexibility as we manipulate in-memory data?\nJoe Definitely.\nTIP In DOP, we represent data from the database with generic data collections, and\nwe manipulate it with generic functions.\nTheo Would you show me how to retrieve book search results when the catalog data\nis stored in an SQL database?\nJoe I’ll show you in a moment. First, tell me how you would design the tables that\nstore catalog data.\nTheo Do you mean the exact table schemas with the information about primary keys\nand nullability of each and every column?\nJoe No, I only need a rough overview of the tables, their columns, and the relation-\nships between the tables.\nTheo goes to the whiteboard. Figure 10.1 shows the diagram he draws as he explains his\nthinking to Joe.\n--- Page 227 ---\n10.1 Fetching data from the database 199\nT books\nT authors\nisbn VARCHAR[32]\nid VARCHAR[64]\ntitle VARCHAR[64]\nname VARCHAR[64]\npublication_year INTEGER\n1\nA book 1 An author\nmay have may author\nmany authors. many books.\n* *\nbook_authors\nT (relationships of books and authors)\nbook_isbn VARCHAR[32] Figure 10.1 The database model\nauthor_id VARCHAR[64]\nfor books and authors\nTheo I have a books table with three columns: title, isbn, and publication_\nyear. I also have an authors table with two columns: for id and name. Here,\nlet me draw these tables on the whiteboard to give you a visual (see tables 10.1\nand 10.2).\nTable 10.1 The books table filled with two books\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nTable 10.2 The authors table filled with three authors\nid name\nsean-covey Sean Covey\nstephen-covey Stephen Covey\ncharles-duhigg Charles Duhigg\nJoe What about the connection between books and authors?\nTheo Let’s see, a book could be written by multiple authors, and an author could write\nmultiple books. Therefore, I need a many-to-many book_authors table that con-\nnects authors and books with two columns, book_isbn and author_id.\nTheo once again turns to the whiteboard. He pens the book_authors table 10.3 to show Joe.\nTable 10.3 The book_authors table with rows connecting books with their authors\nbook_isbn author_id\n978-1982137274 sean-covey\n978-1982137274 stephen-covey\n978-0812981605 charles-duhigg\n--- Page 228 ---\n200 CHAPTER 10 Database operations\nJoe Great! Let’s start with the simplest case. We’re going to write code that searches\nfor books matching a title and that returns basic information about the books.\nBy basic information, I mean title, ISBN, and publication year.\nTheo What about the book authors?\nJoe We’ll deal with that later, as it’s a bit more complicated. Can you write an SQL\nquery for retrieving books that contain he word habit in their title?\nTheo Sure.\nThis assignment is quite easy for Theo. First, he jots down the SQL query, then he displays\nthe results in table 10.4.\nListing10.1 SQL query to retrieve books whose title contains habit\nSELECT\ntitle,\nisbn,\npublication_year\nFROM\nbooks\nWHERE title LIKE '%habit%';\nTable 10.4 Results of the SQL query for books whose title contains the word habit\ntitle isbn publication_year\nThe Power of Habit 978-0812981605 2012\n7 Habits of Highly Effective People 978-1982137274 1989\nJoe How would you describe these results as a data collection?\nTheo I would say it’s a list of maps.\nTIP In DOP, accessing data from a NoSQL database is similar to the way we access\ndata from a relational database.\nJoe Right! Now, can you write the search results as a list of maps?\nTheo It doesn’t sound too complicated. How about this?\nListing10.2 Search results as a list of maps\n[\n{\n\"title\": \"7 Habits of Highly Effective People\",\n\"isbn\": \"978-1982137274\",\n\"publication_year\": 1989\n},\n{\n\"title\": \"The Power of Habit\",\n\"isbn\": \"978-0812981605\",\n\"publication_year\": 2012\n}\n]\n--- Page 229 ---\n10.1 Fetching data from the database 201\nJoe What about the JSON schema for the search results?\nTheo It shouldn’t be too difficult if you allow me to take a look at the JSON schema\ncheat sheet you kindly offered me the other day.\nJoe Of course. The purpose of a gift is to be used by the one who receives it.\nTheo takes a look at the JSON Schema cheat sheet to refresh his memory about the JSON\nSchema syntax. After a few minutes, Theo comes up with a schema for the search results.\nHe certainly is putting Joe’s gift to good use.\nListing10.3 JSON schema cheat sheet\n{\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"myNumber\": {\"type\": \"number\"},\n\"myString\": {\"type\": \"string\"},\n\"myEnum\": {\"enum\": [\"myVal\", \"yourVal\"]},\n\"myBool\": {\"type\": \"boolean\"}\n},\n\"required\": [\"myNumber\", \"myString\"],\n\"additionalProperties\": false\n}\n}\nListing10.4 The JSON schema for search results from the database\nvar dbSearchResultSchema = {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"required\": [\"title\", \"isbn\", \"publication_year\"],\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"isbn\": {\"type\": \"string\"},\n\"publication_year\": {\"type\": \"integer\"}\n}\n}\n};\nJoe Excellent. Now I’m going to show you how to implement searchBooks in a\nway that fetches data from the database and returns a JSON string with the\nresults. The cool thing is that we’re only going to use generic data collections\nfrom the database layer down to the JSON serialization.\nTheo Will it be similar to the implementation of searchBooks that we wrote when\nyou taught me the basis of DOP?\nJoe Absolutely. The only difference is that then the state of the system was stored\nlocally, and we queried it with a function like _.filter. Now, we use SQL\n--- Page 230 ---\n202 CHAPTER 10 Database operations\nqueries to fetch the state from the database. In terms of data representation\nand manipulation, it’s exactly the same.\nJoe goes to the whiteboard and sketches out the data flow in figure 10.2. Theo studies the\ndiagram.\nDatabase\nDatabase driver\nData (list of maps)\nData manipulation\nData Figure 10.2 Data flow for serving\na request that fetches data from\nJSON serialize\nthe database\nJoe The data manipulation step in the diagram is implemented via generic func-\ntions that manipulate data collections. As our examples get more elaborate, I\nthink you’ll see the benefits of being able to manipulate data collections with\ngeneric functions.\nTheo Sounds intriguing...\nJoe For the communication with the database, we use a driver that returns a list of\nmaps. In JavaScript, you could use an SQL driver like node-postgres.\n NOTE See https://node-postgres.com for more information about this collection of\nnode.js modules for interfacing with PostgreSQL databases.\nTheo And in Java?\nJoe In Java, you could use JDBC (Java database connectivity) in addition to a small\nutility function that converts a JDBC result set into a list of maps. If I can use\nyour laptop, I’ll show you what I mean.\nJoe pulls a piece of code from one of his personal GitHub repositories. He then shows the\ncode for the JDBC conversion to Theo, who seems a bit surprised.\nListing10.5 Converting a JDBC result set into a list of hash maps\nList<Map<String, Object>> convertJDBCResultSetToListOfMaps(ResultSet rs) {\nList<Map<String, Object>> listOfMaps =\nnew ArrayList<Map<String, Object>>();\nResultSetMetaData meta = rs.getMetaData();\nwhile (rs.next()) {\nMap map = new HashMap();\nfor (int i = 1; i <= meta.getColumnCount(); i++) {\nString key = meta.getColumnLabel(i);\nObject value = rs.getObject(i);\n--- Page 231 ---\n10.1 Fetching data from the database 203\nmap.put(key, value);\n}\nlistOfMaps.add(map);\n}\nreturn listOfMaps;\n}\nTIP Converting a JDBC result set into a list of hash maps is quite straightforward.\nTheo I expected it to be much more complicated to convert a JDBC result set into a\nlist of hash maps.\nJoe It’s straightforward because, in a sense, JDBC is data-oriented.\nTheo What about the field types?\nJoe When we convert a JDBC result set into a list of maps, each value is considered\nan Object.\nTheo That’s annoying because it means that in order to access the value, we need to\ncast it to its type.\nJoe Yes and no. Look at our book search use case. We pass all the values along with-\nout really looking at their type. The concrete value type only matters when we\nserialize the result into JSON and that’s handled by the JSON serialization\nlibrary. It’s called late binding.\n NOTE With late binding, we defer dealing with data types as long as possible.\nTheo Does that mean in my application that I’m allowed to manipulate data without\ndealing with concrete types?\nTIP In DOP, flexibility is increased as many parts of the system are free to manipulate\ndata without dealing with concrete types.\nJoe Exactly. You’ll see late binding in action in a moment. That’s one of the great-\nest benefits of DOP.\nTheo Interesting, I can’t wait to see that!\nJoe One last thing before I show you the code for retrieving search results from the\ndatabase. In order to make it easier to read, I’m going to write JavaScript code\nas if JavaScript were dealing with I/O is a synchronous way.\nTheo What do you mean?\nJoe In JavaScript, an I/O operation like sending a query to the database is done\nasynchronously. In real life, it means using either callback functions or using\nasync and await keywords.\nTheo Oh yeah, that’s because JavaScript is single-threaded.\n NOTE For sake of simplicity, the JavaScript snippets in this chapter are written as if\nJavaScript were dealing with I/O in a synchronous way. In real-life JavaScript, we need\nto use async and await around I/O calls.\nJoe Indeed, so I’ll be writing the code that communicates with the database as\nthough JavaScript were dealing with I/O synchronously. Here’s an example.\n--- Page 232 ---\n204 CHAPTER 10 Database operations\nListing10.6 Searching books in the database, returning the results in JSON\ndbClient holds the Initializes Ajv (a JSON schema validation\nvar dbClient; DB connection. library) with allErrors: true to catch all\nthe data validation errors\nvar ajv = new Ajv({allErrors: true});\nvar title = \"habit\";\nvar matchingBooksQuery = `SELECT title, isbn Uses a parameterized\nSQL query as a security\nFROM books\nbest practice\nWHERE title LIKE '%$1%'`;\nvar books = dbClient.query(matchingBooksQuery,\nPasses the parameters to the SQL\n[title]);\nquery as a list of values (in our\nif(!ajv.validate(dbSearchResultSchema, books)) {\ncase, a list with a single value)\nvar errors = ajv.errorsText(ajv.errors);\nthrow \"Internal error: Unexpected result from the database: \" + errors;\n}\nJSON.stringify(books);\nTheo In a dynamically-typed language like JavaScript, I understand that the types of\nthe values in the list of maps returned by dbClient.query are determined at\nrun time. How does it work in a statically-typed language like Java, and what are\nthe types of the data fields in books?\nJoe The function convertJDBCResultSetToListOfMaps we created earlier (see\nlisting 10.5) returns a list of Map<String, Object>. But JSON serialization\nlibraries like Gson know how to detect at run time the concrete type of the val-\nues in a map and serialize the values according to their type.\n NOTE See https://github.com/google/gson for information about Gson’s Java\nserialization/deserialization library.\nTheo What do you mean by serializing a value according to its type?\nJoe For instance, the value of the field publication_year is a number; therefore,\nit is not wrapped with quotes. However, the value of the field title is a string;\ntherefore, it is wrapped with quotes.\nTheo Nice! Now, I understand what you mean by late binding.\nJoe Cool! Now, let me show you how we store data in the database.\n10.2 Storing data in the database\nIn the previous section, we saw how to retrieve data from the database as a list of maps.\nNext, we’ll see how to store data in the database when data is represented with a map.\nTheo I guess that storing data in the database is quite similar to fetching data from\nthe database.\nJoe It’s similar in the sense that we deal only with generic data collections. Can you\nwrite a parameterized SQL query that inserts a row with user info using only\nemail and encrypted_password, please?\nTheo OK.",
        "sections_found": []
      },
      "accurate_page_range": "202-232"
    }
  ]
}